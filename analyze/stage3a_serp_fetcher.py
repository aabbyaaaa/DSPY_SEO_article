# -*- coding: utf-8 -*-
"""
Stage 3a: SERP Analysis Module (Bilingual)
-------------------------------------------
æ”¯æ´é›™èªž SERP åˆ†æžï¼ˆç¹é«”ä¸­æ–‡ + è‹±æ–‡ï¼‰
å¾ž Stage 2 èªžç¾©è©•åˆ†çµæžœä¸­ç¯©é¸ High + Medium Tier æŸ¥è©¢ï¼ŒæŠ“å– SERP è³‡æ–™
"""

import os, json, time, sys, io
from pathlib import Path
import pandas as pd
from serpapi import GoogleSearch
from tqdm import tqdm

# Windows UTF-8 support
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config

print("\n" + "=" * 60)
print("ðŸŒ é›™èªž SERP åˆ†æžæ¨¡çµ„")
print("=" * 60)
print(f"ä¸­æ–‡ä¸»é¡Œï¼š{config.topic}")
print(f"è‹±æ–‡ä¸»é¡Œï¼š{config.topic_en}")
print(f"SERP Top N: {config.serp_top_n}")

SERPAPI_KEY = config.get_serpapi_key()

def fetch_serp(query: str, lang: str = "zh-TW") -> dict:
    """
    Fetch Google SERP data using SerpAPI

    Args:
        query: æŸ¥è©¢è©ž
        lang: èªžè¨€ (zh-TW æˆ– en)

    Returns:
        SERP è³‡æ–™å­—å…¸
    """

    # æ ¹æ“šèªžè¨€è¨­å®šåƒæ•¸
    if lang == "zh-TW":
        location = "Taiwan"
        hl = "zh-TW"
        gl = "tw"
    else:
        location = "United States"
        hl = "en"
        gl = "us"

    params = {
        "q": query,
        "location": location,
        "hl": hl,
        "gl": gl,
        "api_key": SERPAPI_KEY,
        "num": config.serp_top_n
    }

    try:
        search = GoogleSearch(params)
        results = search.get_dict()

        # Extract AI Overview
        ai_overview = {
            "present": "answer_box" in results or "ai_overview" in results,
            "content": ""
        }

        if "answer_box" in results:
            ai_overview["content"] = results["answer_box"].get("snippet", "")
        elif "ai_overview" in results:
            ai_overview["content"] = results["ai_overview"].get("snippet", "")

        # Extract organic results
        organic = []
        for i, item in enumerate(results.get("organic_results", [])[:config.serp_top_n], 1):
            organic.append({
                "position": i,
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", ""),
                "domain": item.get("displayed_link", item.get("link", "").split('/')[2] if '/' in item.get("link", "") else "")
            })

        # Extract People Also Ask
        paa = []
        if config.serp_extract_paa:
            for item in results.get("related_questions", []):
                paa.append({
                    "question": item.get("question", ""),
                    "answer": item.get("snippet", "")
                })

        # Extract Related Searches
        related = []
        if config.serp_extract_related:
            for item in results.get("related_searches", []):
                related.append(item.get("query", ""))

        return {
            "query": query,
            "lang": lang,
            "ai_overview": ai_overview,
            "organic_results": organic,
            "people_also_ask": paa,
            "related_searches": related,
            "total_results": results.get("search_information", {}).get("total_results", 0)
        }

    except Exception as e:
        print(f"\nâš ï¸ SERP fetch error: {query} - {e}")
        return {
            "query": query,
            "lang": lang,
            "error": str(e),
            "ai_overview": {"present": False, "content": ""},
            "organic_results": [],
            "people_also_ask": [],
            "related_searches": [],
            "total_results": 0
        }

def analyze_serp_features(serp_data: dict) -> dict:
    """
    åˆ†æž SERP ç‰¹å¾µï¼Œæª¢æ¸¬ AI å¼•æ“Žå„ªåŒ–ä¿¡è™Ÿ

    æª¢æ¸¬é …ç›®:
    - AIO (AI Overview): Google å°ˆå±¬ AI æ‘˜è¦
    - AEO (Answer Engine): Perplexity, Bing Copilot ç­‰
    - GEO (Generative Engine): ChatGPT, Claude, Gemini å¼•ç”¨
    """

    analysis = {
        "aio_triggered": serp_data["ai_overview"]["present"],  # AIO: Google AI Overview
        "aiseo_triggered": serp_data["ai_overview"]["present"],  # ä¿ç•™å‘å¾Œå…¼å®¹
        "avg_title_length": 0,
        "avg_snippet_length": 0,
        "unique_domains": 0,
        "paa_count": len(serp_data["people_also_ask"]),
        "related_count": len(serp_data["related_searches"]),
        "content_gap_signals": []
    }

    organic = serp_data["organic_results"]
    if organic:
        titles = [len(r["title"]) for r in organic if r["title"]]
        snippets = [len(r["snippet"]) for r in organic if r["snippet"]]

        analysis["avg_title_length"] = round(sum(titles) / len(titles), 1) if titles else 0
        analysis["avg_snippet_length"] = round(sum(snippets) / len(snippets), 1) if snippets else 0
        analysis["unique_domains"] = len(set(r["domain"] for r in organic if r["domain"]))

    # Content gap signals
    if analysis["paa_count"] > 5:
        analysis["content_gap_signals"].append("High PAA count - users have many questions")

    if analysis["aio_triggered"]:
        analysis["content_gap_signals"].append("AIO triggered - Google AI Overview optimization needed")
        analysis["content_gap_signals"].append("Consider AEO strategies - Answer Engine Optimization")

    if analysis["unique_domains"] < 5:
        analysis["content_gap_signals"].append("Low domain diversity - ranking opportunity")

    return analysis

def main():
    # è®€å–èªžç¾©è©•åˆ†çµæžœ
    scores_path = config.data_dir / "semantic_scores.csv"
    if not scores_path.exists():
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {scores_path}ï¼Œè«‹å…ˆåŸ·è¡Œ analyze/stage2_semantic_scoring.py")

    df_all = pd.read_csv(scores_path)
    print(f"\nðŸ“‚ è¼‰å…¥èªžç¾©è©•åˆ†çµæžœï¼š{len(df_all)} å€‹æŸ¥è©¢")

    # ç¯©é¸ High + Medium Tierï¼ˆè·³éŽ Low Tierï¼‰
    df = df_all[df_all["quality_tier"].isin(["high", "medium"])].copy()

    high_count = len(df_all[df_all["quality_tier"] == "high"])
    medium_count = len(df_all[df_all["quality_tier"] == "medium"])
    low_count = len(df_all[df_all["quality_tier"] == "low"])

    print(f"\nðŸ“Š å“è³ªåˆ†ç´šçµ±è¨ˆï¼š")
    print(f"   âœ… High Tier: {high_count} å€‹")
    print(f"   âš ï¸ Medium Tier: {medium_count} å€‹")
    print(f"   âŒ Low Tier: {low_count} å€‹ï¼ˆè·³éŽï¼‰")

    print(f"\nðŸŽ¯ å°‡è™•ç† {len(df)} å€‹æŸ¥è©¢ï¼ˆHigh + Mediumï¼‰")

    zh_count = len(df[df["lang"] == "zh-TW"])
    en_count = len(df[df["lang"] == "en"])
    print(f"   ç¹é«”ä¸­æ–‡ï¼š{zh_count} å€‹")
    print(f"   Englishï¼š{en_count} å€‹")

    # Cache
    cache_path = config.data_dir / "cache_serp_bilingual.json"
    cache = {}
    if cache_path.exists():
        with open(cache_path, "r", encoding="utf-8") as f:
            cache = json.load(f)
        print(f"\nðŸ“¦ è¼‰å…¥ {len(cache)} å€‹å¿«å–çš„ SERP çµæžœ")

    # Fetch SERP data
    serp_results = []
    print("\nðŸ” é–‹å§‹æŠ“å– SERP è³‡æ–™...")

    for _, row in tqdm(df.iterrows(), total=len(df), desc="SERP Fetching"):
        query = row["query"]
        lang = row["lang"]

        cache_key = f"{query}_{lang}"

        if cache_key in cache:
            serp_data = cache[cache_key]
        else:
            serp_data = fetch_serp(query, lang)
            cache[cache_key] = serp_data
            time.sleep(1)  # API é™æµ

        features = analyze_serp_features(serp_data)

        serp_results.append({
            "query": query,
            "lang": lang,
            "serp_data": serp_data,
            "analysis": features
        })

    # Save cache
    print("\nðŸ’¾ å„²å­˜å¿«å–...")
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

    # Output analysis
    output_path = config.data_dir / "serp_analysis_bilingual.json"

    # è¨ˆç®—çµ±è¨ˆ
    aio_trigger_rate = sum(r["analysis"]["aio_triggered"] for r in serp_results) / len(serp_results)
    aiseo_trigger_rate = aio_trigger_rate  # ä¿ç•™å‘å¾Œå…¼å®¹
    avg_paa = sum(r["analysis"]["paa_count"] for r in serp_results) / len(serp_results)
    total_gaps = sum(len(r["analysis"]["content_gap_signals"]) for r in serp_results)

    # åˆ†èªžè¨€çµ±è¨ˆ
    zh_results = [r for r in serp_results if r["lang"] == "zh-TW"]
    en_results = [r for r in serp_results if r["lang"] == "en"]

    zh_aio = sum(r["analysis"]["aio_triggered"] for r in zh_results) / len(zh_results) if zh_results else 0
    en_aio = sum(r["analysis"]["aio_triggered"] for r in en_results) / len(en_results) if en_results else 0

    # ä¿ç•™èˆŠè®Šæ•¸åä»¥å‘å¾Œå…¼å®¹
    zh_aiseo = zh_aio
    en_aiseo = en_aio

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "topic_zh": config.topic,
            "topic_en": config.topic_en,
            "query_count": len(df),
            "serp_results": serp_results,
            "summary": {
                "total_queries": len(serp_results),
                "zh_queries": len(zh_results),
                "en_queries": len(en_results),
                "aio_trigger_rate": aio_trigger_rate,  # AIO (Google AI Overview)
                "zh_aio_rate": zh_aio,
                "en_aio_rate": en_aio,
                # ä¿ç•™èˆŠæ¬„ä½ä»¥å‘å¾Œå…¼å®¹
                "aiseo_trigger_rate": aiseo_trigger_rate,
                "zh_aiseo_rate": zh_aiseo,
                "en_aiseo_rate": en_aiseo,
                "avg_paa_per_query": avg_paa,
                "total_content_gaps": total_gaps
            }
        }, f, ensure_ascii=False, indent=2)

    # çµæžœå ±å‘Š
    print("\n" + "=" * 60)
    print("âœ… SERP åˆ†æžå®Œæˆï¼")
    print("=" * 60)
    print(f"ðŸ“ è¼¸å‡ºæª”æ¡ˆï¼š{output_path}")
    print(f"ðŸ“ å¿«å–æª”æ¡ˆï¼š{cache_path}")
    print(f"\nðŸ“Š çµ±è¨ˆè³‡è¨Šï¼š")
    print(f"   ç¸½æŸ¥è©¢æ•¸ï¼š{len(serp_results)}")
    print(f"   ç¹é«”ä¸­æ–‡ï¼š{len(zh_results)}")
    print(f"   Englishï¼š{len(en_results)}")
    print(f"\nðŸ¤– AIO (Google AI Overview) è§¸ç™¼çŽ‡ï¼š")
    print(f"   æ•´é«”ï¼š{aio_trigger_rate:.1%}")
    print(f"   ä¸­æ–‡ï¼š{zh_aio:.1%}")
    print(f"   è‹±æ–‡ï¼š{en_aio:.1%}")
    print(f"\nâ“ å¹³å‡ PAA æ•¸é‡ï¼š{avg_paa:.1f}")
    print(f"ðŸ” å…§å®¹ç¼ºå£ä¿¡è™Ÿç¸½æ•¸ï¼š{total_gaps}")
    print("\n" + "=" * 60)

if __name__ == "__main__":
    main()
