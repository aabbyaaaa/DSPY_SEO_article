# -*- coding: utf-8 -*-
"""
LLM-SEO Pipeline Stage â‘¢: DSPy Analysis Runner (v2.0 - Bilingual with Full Content)
------------------------------------------------------------------------------------
åŸ·è¡Œ DSPy åˆ†æï¼Œä½¿ç”¨å®Œæ•´é é¢å…§å®¹è€Œéåƒ… SERP snippets

ç‰¹é»ï¼š
1. æ”¯æ´é›™èªï¼ˆç¹é«”ä¸­æ–‡ + è‹±æ–‡ï¼‰
2. ä½¿ç”¨ Tavily æå–çš„å®Œæ•´é é¢å…§å®¹
3. ç‚ºæ¯å€‹æŸ¥è©¢ç”Ÿæˆæ–‡ç« å¤§ç¶±

è¼¸å…¥ï¼š
- data/serp_analysis_bilingual.json
- data/extracted_content_60_pages.json

è¼¸å‡ºï¼š
- data/article_outlines_bilingual.json
"""

import os, json, sys, io
from pathlib import Path
from tqdm import tqdm

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config
from analyze.dspy_modules import (
    ContentSummarizer,
    GapAnalyzer,
    OutlineGenerator,
    init_dspy
)

# ================================================
# åˆå§‹åŒ–
# ================================================
print("\n" + "=" * 60)
print("ğŸš€ DSPy é›™èªåˆ†ææ¨¡çµ„ - Stage â‘¢ (v2.0)")
print("=" * 60)
print(f"ä¸­æ–‡ä¸»é¡Œï¼š{config.topic}")
print(f"è‹±æ–‡ä¸»é¡Œï¼š{config.topic_en}")
print(f"DSPy ä¸»æ¨¡å‹ï¼š{config.dspy_model_main}")
print(f"DSPy å°æ¨¡å‹ï¼š{config.dspy_model_small}")

# åˆå§‹åŒ– DSPy
openai_key = config.get_openai_key()
init_dspy(config.dspy_model_main, openai_key)

# åˆå§‹åŒ–ä¸‰å€‹æ¨¡çµ„
print("\nğŸ“¦ åˆå§‹åŒ– DSPy æ¨¡çµ„...")
summarizer = ContentSummarizer()
gap_analyzer = GapAnalyzer()
outline_generator = OutlineGenerator(block_config=config.article_blocks)

print("âœ… æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")

# ================================================
# è¼‰å…¥è³‡æ–™
# ================================================

# 1. SERP åˆ†æè³‡æ–™
serp_path = config.data_dir / "serp_analysis_bilingual.json"
if not serp_path.exists():
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {serp_path}ï¼Œè«‹å…ˆåŸ·è¡Œ analyze/serp_fetcher_bilingual.py")

print(f"\nğŸ“– è¼‰å…¥ SERP è³‡æ–™ï¼š{serp_path}")
with open(serp_path, "r", encoding="utf-8") as f:
    serp_data = json.load(f)

# 2. æå–çš„å®Œæ•´å…§å®¹
content_path = config.data_dir / "extracted_content_60_pages.json"
if not content_path.exists():
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {content_path}ï¼Œè«‹å…ˆåŸ·è¡Œ analyze/content_extractor.py")

print(f"ğŸ“– è¼‰å…¥æå–çš„é é¢å…§å®¹ï¼š{content_path}")
with open(content_path, "r", encoding="utf-8") as f:
    content_data = json.load(f)

query_count = len(serp_data["serp_results"])
page_count = len(content_data["pages"])

print(f"âœ… è¼‰å…¥ {query_count} æ¢æŸ¥è©¢çš„ SERP è³‡æ–™")
print(f"âœ… è¼‰å…¥ {page_count} å€‹é é¢çš„å®Œæ•´å…§å®¹")

# ================================================
# å»ºç«‹æŸ¥è©¢ -> å…§å®¹çš„æ˜ å°„
# ================================================

print("\nğŸ”— å»ºç«‹æŸ¥è©¢èˆ‡å…§å®¹çš„æ˜ å°„...")
query_content_map = {}

for page in content_data["pages"]:
    query = page["query"]
    if query not in query_content_map:
        query_content_map[query] = []
    query_content_map[query].append(page)

print(f"âœ… {len(query_content_map)} å€‹æŸ¥è©¢æœ‰å°æ‡‰çš„å…§å®¹")

# ================================================
# è™•ç†æ¯å€‹æŸ¥è©¢
# ================================================
results = []

print("\n" + "=" * 60)
print("ğŸ”„ é–‹å§‹è™•ç†æŸ¥è©¢...")
print("=" * 60)

for idx, item in enumerate(tqdm(serp_data["serp_results"], desc="DSPy åˆ†æ"), 1):
    query = item["query"]
    lang = item["lang"]
    serp = item["serp_data"]
    analysis = item["analysis"]

    lang_label = "ç¹ä¸­" if lang == "zh-TW" else "è‹±æ–‡"
    print(f"\n[{idx}/{query_count}] ({lang_label}) {query}")

    # ------------------------------------------------
    # Step 1ï¸âƒ£: ContentSummarizer
    # ------------------------------------------------
    print("  ğŸ“ Step 1: ç¸½çµç«¶çˆ­è€…å…§å®¹...")
    organic_results = serp.get("organic_results", [])

    if not organic_results:
        print("  âš ï¸ ç„¡æœ‰æ©Ÿçµæœï¼Œè·³é")
        continue

    # å¢å¼· organic_resultsï¼šå¦‚æœæœ‰å®Œæ•´å…§å®¹ï¼ŒåŠ å…¥æ‘˜è¦
    pages = query_content_map.get(query, [])

    # ç‚ºæ¯å€‹ organic result æ·»åŠ å®Œæ•´å…§å®¹ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    for result in organic_results:
        result_url = result.get("link", "")
        # å°‹æ‰¾å°æ‡‰çš„å®Œæ•´é é¢å…§å®¹
        matching_page = next((p for p in pages if p["url"] == result_url), None)
        if matching_page:
            # æˆªå–å‰ 500 å­—ä½œç‚ºå¢å¼·çš„ snippet
            content_preview = matching_page["content"][:500]
            result["enhanced_snippet"] = f"{result.get('snippet', '')} [å®Œæ•´å…§å®¹é è¦½: {content_preview}...]"
        else:
            result["enhanced_snippet"] = result.get("snippet", "")

    try:
        summaries = summarizer.forward(query, organic_results)
        print(f"  âœ… ç¸½çµå®Œæˆï¼š{len(summaries)} å€‹ç«¶çˆ­è€…")
    except Exception as e:
        print(f"  âŒ ContentSummarizer å¤±æ•—ï¼š{e}")
        continue

    # ------------------------------------------------
    # Step 2ï¸âƒ£: GapAnalyzer
    # ------------------------------------------------
    print("  ğŸ” Step 2: åˆ†æå…§å®¹ç¼ºå£...")
    paa_questions = serp.get("people_also_ask", [])
    # AIO (Google AI Overview) æª¢æ¸¬
    aio_triggered = analysis.get("aio_triggered", analysis.get("aiseo_triggered", False))

    try:
        gaps = gap_analyzer.forward(
            query=query,
            competitor_summaries=summaries,
            paa_questions=paa_questions,
            aiseo_triggered=aio_triggered  # å‚³å…¥ AIO ç‹€æ…‹
        )
        print(f"  âœ… ç¼ºå£åˆ†æå®Œæˆï¼šæ‰¾åˆ° {len(gaps)} å€‹æ©Ÿæœƒ")

        # é¡¯ç¤ºå‰ 3 å€‹ç¼ºå£
        for i, gap in enumerate(gaps[:3], 1):
            gap_desc = gap.description[:50] if len(gap.description) > 50 else gap.description
            print(f"     {i}. [{gap.gap_type}] {gap_desc}... (åˆ†æ•¸: {gap.opportunity_score:.2f})")

    except Exception as e:
        print(f"  âŒ GapAnalyzer å¤±æ•—ï¼š{e}")
        gaps = []

    # ------------------------------------------------
    # Step 3ï¸âƒ£: OutlineGenerator
    # ------------------------------------------------
    print("  ğŸ“‹ Step 3: ç”Ÿæˆæ–‡ç« å¤§ç¶±...")

    try:
        outline = outline_generator.forward(
            query=query,
            content_gaps=gaps,
            paa_questions=paa_questions,
            aiseo_triggered=aio_triggered  # å‚³å…¥ AIO ç‹€æ…‹
        )
        print(f"  âœ… å¤§ç¶±ç”Ÿæˆå®Œæˆ")

        # é¡¯ç¤ºå€å¡Šçµæ§‹
        if "blocks" in outline:
            print(f"     çµæ§‹ï¼š{len(outline['blocks'])} å€‹å€å¡Š")
            for block in outline["blocks"]:
                print(f"       - {block.get('block_name', 'N/A')}: {block.get('word_count_target', 'N/A')} å­—")

    except Exception as e:
        print(f"  âŒ OutlineGenerator å¤±æ•—ï¼š{e}")
        outline = outline_generator._default_outline(query)

    # ------------------------------------------------
    # å„²å­˜çµæœ
    # ------------------------------------------------
    results.append({
        "query": query,
        "lang": lang,
        "aio_triggered": aio_triggered,  # AIO (Google AI Overview)
        "aiseo_triggered": aio_triggered,  # ä¿ç•™å‘å¾Œå…¼å®¹
        "competitor_summaries": [s.model_dump() for s in summaries],
        "content_gaps": [g.model_dump() for g in gaps],
        "outline": outline,
        "paa_questions": paa_questions,
        "related_searches": serp.get("related_searches", []),
        "extracted_pages": [
            {
                "url": p["url"],
                "title": p["title"],
                "content_length": len(p["content"])
            }
            for p in pages
        ]
    })

# ================================================
# è¼¸å‡ºæœ€çµ‚çµæœ
# ================================================
output_path = config.data_dir / "article_outlines_bilingual.json"

# è¨ˆç®—çµ±è¨ˆ
total_gaps = sum(len(r["content_gaps"]) for r in results)
avg_gaps = total_gaps / len(results) if results else 0
aio_coverage = sum(r["aio_triggered"] for r in results) / len(results) if results else 0
aiseo_coverage = aio_coverage  # ä¿ç•™å‘å¾Œå…¼å®¹

zh_results = [r for r in results if r["lang"] == "zh-TW"]
en_results = [r for r in results if r["lang"] == "en"]

with open(output_path, "w", encoding="utf-8") as f:
    json.dump({
        "topic_zh": config.topic,
        "topic_en": config.topic_en,
        "query_count": len(results),
        "zh_count": len(zh_results),
        "en_count": len(en_results),
        "outlines": results,
        "summary": {
            "total_gaps_found": total_gaps,
            "avg_gaps_per_query": avg_gaps,
            "aio_coverage": aio_coverage,  # AIO (Google AI Overview)
            "zh_aio_rate": sum(r["aio_triggered"] for r in zh_results) / len(zh_results) if zh_results else 0,
            "en_aio_rate": sum(r["aio_triggered"] for r in en_results) / len(en_results) if en_results else 0,
            # ä¿ç•™èˆŠæ¬„ä½ä»¥å‘å¾Œå…¼å®¹
            "aiseo_coverage": aiseo_coverage,
            "zh_aiseo_rate": sum(r["aiseo_triggered"] for r in zh_results) / len(zh_results) if zh_results else 0,
            "en_aiseo_rate": sum(r["aiseo_triggered"] for r in en_results) / len(en_results) if en_results else 0
        }
    }, f, ensure_ascii=False, indent=2)

print("\n" + "=" * 60)
print("âœ… DSPy é›™èªåˆ†æå®Œæˆï¼")
print("=" * 60)
print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š{output_path}")
print(f"\nğŸ“Š è™•ç†çµ±è¨ˆï¼š")
print(f"   ç¸½æŸ¥è©¢æ•¸ï¼š{len(results)}")
print(f"   ç¹é«”ä¸­æ–‡ï¼š{len(zh_results)}")
print(f"   Englishï¼š{len(en_results)}")
print(f"\nğŸ¯ å…§å®¹ç¼ºå£ï¼š")
print(f"   ç¸½ç¼ºå£æ•¸ï¼š{total_gaps}")
print(f"   å¹³å‡ç¼ºå£/æŸ¥è©¢ï¼š{avg_gaps:.1f}")
print(f"\nğŸ¤– AIO (Google AI Overview) è¦†è“‹ç‡ï¼š")
print(f"   æ•´é«”ï¼š{aio_coverage:.1%}")
print(f"   ä¸­æ–‡ï¼š{sum(r['aio_triggered'] for r in zh_results) / len(zh_results):.1%}" if zh_results else "   ä¸­æ–‡ï¼šN/A")
print(f"   è‹±æ–‡ï¼š{sum(r['aio_triggered'] for r in en_results) / len(en_results):.1%}" if en_results else "   è‹±æ–‡ï¼šN/A")
print("\nğŸ‰ æº–å‚™é€²å…¥ Stage â‘£ï¼šæ–‡ç« ç”Ÿæˆ")
print("=" * 60 + "\n")
