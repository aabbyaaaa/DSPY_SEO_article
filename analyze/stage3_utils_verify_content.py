# -*- coding: utf-8 -*-
"""
é©—è­‰æå–å…§å®¹çš„å¯¦éš›ä¸»é¡Œåˆ†å¸ƒ
"""
import json
import sys
import io
from pathlib import Path
from collections import Counter

if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config

# è®€å–æå–çš„å…§å®¹
data_path = config.data_dir / "extracted_content_top_queries_3.8.json"
with open(data_path, "r", encoding="utf-8") as f:
    data = json.load(f)

print("=" * 80)
print("ğŸ“Š æå–å…§å®¹ä¸»é¡Œåˆ†æ")
print("=" * 80)

# åˆ†ææŸ¥è©¢è©ä¸»é¡Œ
zh_pages = [p for p in data["pages"] if p["lang"] == "zh-TW"]
en_pages = [p for p in data["pages"] if p["lang"] == "en"]

print(f"\nç¸½è¨ˆï¼š{len(data['pages'])} ç¯‡æ–‡ç« ")
print(f"ä¸­æ–‡ï¼š{len(zh_pages)} ç¯‡")
print(f"è‹±æ–‡ï¼š{len(en_pages)} ç¯‡")

# åˆ†æä¸­æ–‡æŸ¥è©¢è©çš„ä¸»é¡Œåˆ†å¸ƒ
print("\n" + "=" * 80)
print("ğŸ” ä¸­æ–‡æŸ¥è©¢è©ä¸»é¡Œåˆ†æ")
print("=" * 80)

query_themes = {
    "æ•…éšœæ’æŸ¥": ["æ•…éšœ", "æ’æŸ¥", "ç•°å¸¸", "å•é¡Œ", "è¨ºæ–·", "æ’é™¤"],
    "ç¶­ä¿®ä¿é¤Š": ["ç¶­ä¿®", "ä¿é¤Š", "ç¶­è­·", "æ¸…æ½”", "æ ¡æ­£"],
    "æ“ä½œä½¿ç”¨": ["ä½¿ç”¨", "æ“ä½œ", "æ­¥é©Ÿ", "æŒ‡å—", "æµç¨‹"],
    "é¸è³¼é…ä»¶": ["é¸è³¼", "æ¨è–¦", "é…ä»¶", "æ›¿ä»£å“", "é¸æ“‡"],
    "åŸç†çŸ¥è­˜": ["åŸç†", "å®šç¾©", "ä»€éº¼æ˜¯", "å¦‚ä½•å·¥ä½œ"],
    "æ¸¬è©¦é©—è­‰": ["æ¸¬è©¦", "é©—è­‰", "ç¢ºæ•ˆ", "ç›£æ¸¬"],
    "ç‰¹å®šæ•…éšœ": ["æ¼æ°´", "æº«åº¦", "å£“åŠ›", "é¡¯ç¤ºå™¨", "æ°´ä½", "å¯†å°"]
}

zh_queries = [p["query"] for p in zh_pages]
unique_zh_queries = sorted(set(zh_queries))

print(f"\nğŸ“Œ å»é‡å¾Œçš„ä¸­æ–‡æŸ¥è©¢è©æ•¸ï¼š{len(unique_zh_queries)}")

for theme, keywords in query_themes.items():
    matching_queries = [q for q in unique_zh_queries if any(kw in q for kw in keywords)]
    print(f"\nã€{theme}ã€‘({len(matching_queries)} å€‹æŸ¥è©¢)")
    for q in matching_queries[:10]:  # é¡¯ç¤ºå‰10å€‹
        count = zh_queries.count(q)
        print(f"  â€¢ {q} ({count} ç¯‡)")
    if len(matching_queries) > 10:
        print(f"  ... é‚„æœ‰ {len(matching_queries) - 10} å€‹")

# åˆ†æè‹±æ–‡æŸ¥è©¢è©
print("\n" + "=" * 80)
print("ğŸ” è‹±æ–‡æŸ¥è©¢è©ä¸»é¡Œåˆ†æ")
print("=" * 80)

en_queries = [p["query"] for p in en_pages]
unique_en_queries = sorted(set(en_queries))

print(f"\nğŸ“Œ å»é‡å¾Œçš„è‹±æ–‡æŸ¥è©¢è©æ•¸ï¼š{len(unique_en_queries)}")

en_themes = {
    "Troubleshooting": ["troubleshoot", "error", "problem", "issue", "malfunction", "fix"],
    "Maintenance": ["maintenance", "maintain", "service", "clean", "calibrat"],
    "Operation": ["use", "operate", "run", "start", "load"],
    "Validation": ["validat", "test", "verify", "indicator"],
    "Specific Issues": ["seal", "pressure", "temperature", "leak", "door"]
}

for theme, keywords in en_themes.items():
    matching_queries = [q for q in unique_en_queries if any(kw in q.lower() for kw in keywords)]
    print(f"\nã€{theme}ã€‘({len(matching_queries)} å€‹æŸ¥è©¢)")
    for q in matching_queries[:8]:
        count = en_queries.count(q)
        print(f"  â€¢ {q} ({count} ç¯‡)")
    if len(matching_queries) > 8:
        print(f"  ... é‚„æœ‰ {len(matching_queries) - 8} å€‹")

# åˆ†ææ–‡ç« æ¨™é¡Œçš„ä¸»é¡Œ
print("\n" + "=" * 80)
print("ğŸ“„ æ–‡ç« æ¨™é¡Œé—œéµå­—åˆ†æï¼ˆä¸­æ–‡å‰20ç¯‡ï¼‰")
print("=" * 80)

for i, page in enumerate(zh_pages[:20], 1):
    title = page.get("title", "")
    query = page.get("query", "")
    url = page.get("url", "")
    quality = page.get("quality_score", 0)
    domain = url.split('/')[2] if '/' in url else url

    print(f"\n{i}. {title[:60]}")
    print(f"   æŸ¥è©¢ï¼š{query}")
    print(f"   ä¾†æºï¼š{domain} (å“è³ªåˆ†æ•¸: {quality})")

    # æª¢æŸ¥æ¨™é¡Œä¸­çš„é—œéµä¸»é¡Œ
    themes_found = []
    for theme, keywords in query_themes.items():
        if any(kw in title for kw in keywords):
            themes_found.append(theme)
    if themes_found:
        print(f"   ä¸»é¡Œï¼š{', '.join(themes_found)}")

# çµ±è¨ˆå…§å®¹é•·åº¦
print("\n" + "=" * 80)
print("ğŸ“Š å…§å®¹çµ±è¨ˆ")
print("=" * 80)

zh_lengths = [len(p.get("content", "")) for p in zh_pages]
en_lengths = [len(p.get("content", "")) for p in en_pages]

print(f"\nä¸­æ–‡æ–‡ç« å¹³å‡é•·åº¦ï¼š{sum(zh_lengths) / len(zh_lengths):.0f} å­—ç¬¦")
print(f"è‹±æ–‡æ–‡ç« å¹³å‡é•·åº¦ï¼š{sum(en_lengths) / len(en_lengths):.0f} å­—ç¬¦")

print(f"\nä¸­æ–‡å…§å®¹é•·åº¦åˆ†å¸ƒï¼š")
print(f"  æœ€çŸ­ï¼š{min(zh_lengths)} å­—ç¬¦")
print(f"  æœ€é•·ï¼š{max(zh_lengths)} å­—ç¬¦")
print(f"  ä¸­ä½æ•¸ï¼š{sorted(zh_lengths)[len(zh_lengths)//2]} å­—ç¬¦")

# åˆ†æä¾†æºç¶²ç«™
print("\n" + "=" * 80)
print("ğŸŒ å…§å®¹ä¾†æºåˆ†æï¼ˆä¸­æ–‡ï¼‰")
print("=" * 80)

domains = [p["url"].split('/')[2] if '/' in p["url"] else p["url"] for p in zh_pages]
domain_counts = Counter(domains)

print(f"\nå‰15å¤§ä¾†æºç¶²ç«™ï¼š")
for domain, count in domain_counts.most_common(15):
    print(f"  {domain}: {count} ç¯‡")

print("\n" + "=" * 80)
