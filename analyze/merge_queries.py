"""
LLM-SEO Pipeline Step 1.5: Merge Queries (Semantic Deduplication)
-----------------------------------------------------------------
ç”¨é€”ï¼š
ä¾æ“šèªç¾©ç›¸ä¼¼åº¦è‡ªå‹•åˆä½µä¸­è‹±æ–‡æˆ–è¿‘ç¾©æŸ¥è©¢ï¼Œ
é¿å…å¾ŒçºŒ semantic_score.py é‡è¤‡è¨ˆç®—æˆ– GPT é‡è¤‡è©•åˆ†ã€‚

è¼¸å…¥ï¼š
  data/query_pool.csv
  data/query_vectors.json

è¼¸å‡ºï¼š
  data/query_pool_merged.csv
  data/query_vectors_merged.json

è¦å‰‡ï¼š
- ä½¿ç”¨ cosine similarity è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦ã€‚
- ç›¸ä¼¼åº¦é–¾å€¼å¾ settings.yaml è¼‰å…¥ã€‚
- ä¿ç•™æ¯ç¾¤ä¸­æœ€çŸ­çš„ä¸­æ–‡æˆ–åŸå§‹æŸ¥è©¢ç‚º main_queryã€‚
"""

import os, json, sys, io, numpy as np, pandas as pd
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config

# =======================================================
# âœ… åŸºæœ¬è¨­å®š
# =======================================================
THRESHOLD = config.merge_threshold  # å¾é…ç½®è¼‰å…¥èªç¾©ç›¸ä¼¼åº¦é–¾å€¼

POOL_PATH = config.data_dir / config.output_files["query_pool"]
VEC_PATH = config.data_dir / config.output_files["query_vectors"]

OUT_CSV = config.data_dir / config.output_files["merged_queries"]
OUT_JSON = config.data_dir / config.output_files["merged_vectors"]

print("Semantic Merge Started...")
print(f"ç›¸ä¼¼åº¦é–¾å€¼ï¼š{THRESHOLD}\n")

# =======================================================
# è¼‰å…¥è³‡æ–™
# =======================================================
if not os.path.exists(POOL_PATH) or not os.path.exists(VEC_PATH):
    raise FileNotFoundError("âŒ æ‰¾ä¸åˆ° query_pool.csv æˆ– query_vectors.jsonï¼Œè«‹å…ˆåŸ·è¡Œ queries.py")

df = pd.read_csv(POOL_PATH)
with open(VEC_PATH, "r", encoding="utf-8") as f:
    vecs = json.load(f)

queries = df["query"].tolist()
vectors = np.array([vecs[q] for q in queries])

print(f"ğŸ“¦ è¼‰å…¥ {len(queries)} æ¢æŸ¥è©¢ï¼Œé–‹å§‹è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦...")

# =======================================================
# å»ºç«‹ç›¸ä¼¼åº¦çŸ©é™£
# =======================================================
sim_matrix = cosine_similarity(vectors)
merged_groups = []
visited = set()

for i, q in enumerate(tqdm(queries, desc="Merging")):
    if q in visited:
        continue

    group = [q]
    visited.add(q)
    for j in range(i + 1, len(queries)):
        if queries[j] in visited:
            continue
        if sim_matrix[i][j] >= THRESHOLD:
            group.append(queries[j])
            visited.add(queries[j])
    merged_groups.append(group)

# =======================================================
# å»ºç«‹åˆä½µçµæœ
# =======================================================
merged_records = []
merged_vectors = {}

for group in merged_groups:
    # ä¿ç•™æœ€çŸ­çš„ä¸­æ–‡ï¼ˆæˆ–ç¬¬ä¸€æ¢ï¼‰ä½œç‚ºä¸»æŸ¥è©¢
    main_query = min(group, key=len)
    synonyms = [q for q in group if q != main_query]

    merged_records.append({
        "main_query": main_query,
        "synonyms": "; ".join(synonyms)
    })

    # å¹³å‡ç¾¤çµ„å‘é‡
    group_vecs = np.array([vecs[q] for q in group])
    merged_vectors[main_query] = group_vecs.mean(axis=0).tolist()

# =======================================================
# è¼¸å‡ºçµæœ
# =======================================================
merged_df = pd.DataFrame(merged_records)
merged_df.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")

with open(OUT_JSON, "w", encoding="utf-8") as f:
    json.dump(merged_vectors, f, ensure_ascii=False, indent=2)

print("\nâœ… èªç¾©åˆä½µå®Œæˆï¼")
print(f"åŸå§‹æŸ¥è©¢ï¼š{len(queries)} â†’ åˆä½µå¾Œï¼š{len(merged_records)}")
print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š")
print(f"   - {OUT_CSV}")
print(f"   - {OUT_JSON}")
