# -*- coding: utf-8 -*-
"""
Stage 3b: å…§å®¹æå–æ¨¡çµ„ (Content Extractor)
------------------------------------------
å¾ SERP çµæœä¸­ç¯©é¸é«˜å“è³ªé é¢ä¸¦ä½¿ç”¨ Tavily Extract API æå–å®Œæ•´å…§å®¹

åŠŸèƒ½ï¼š
1. è®€å– SERP åˆ†æçµæœ
2. ä½¿ç”¨ stage3_utils_page_filter.py ç¯©é¸æ¯å€‹æŸ¥è©¢çš„ Top 3 é«˜å“è³ªé é¢
3. ä½¿ç”¨ Tavily Extract API æå–å®Œæ•´é é¢å…§å®¹
4. å„²å­˜é«˜å“è³ªé é¢å…§å®¹ä¾› Stage 3c DSPy åˆ†æä½¿ç”¨
"""

import os
import sys
import io
import json
import time
from pathlib import Path
from typing import List, Dict
import requests
from tqdm import tqdm

# Windows UTF-8 support
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config
from analyze.stage3_utils_page_filter import filter_organic_results

print("\n" + "=" * 60)
print("ğŸ“¥ å…§å®¹æå–æ¨¡çµ„")
print("=" * 60)

TAVILY_API_KEY = config.get_tavily_key()

def extract_content_tavily(url: str) -> dict:
    """
    ä½¿ç”¨ Tavily Extract API æå–é é¢å…§å®¹

    Args:
        url: é é¢ URL

    Returns:
        æå–çš„å…§å®¹å­—å…¸
    """
    try:
        response = requests.post(
            "https://api.tavily.com/extract",
            json={
                "api_key": TAVILY_API_KEY,
                "urls": [url]
            },
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            if data.get("results") and len(data["results"]) > 0:
                result = data["results"][0]
                return {
                    "url": url,
                    "success": True,
                    "raw_content": result.get("raw_content", ""),
                    "title": result.get("title", ""),
                    "extracted_at": time.strftime("%Y-%m-%d %H:%M:%S")
                }

        return {
            "url": url,
            "success": False,
            "error": f"HTTP {response.status_code}: {response.text[:200]}"
        }

    except Exception as e:
        return {
            "url": url,
            "success": False,
            "error": str(e)
        }

def process_query_pages(query: str, lang: str, organic_results: List[dict], top_n: int = 3) -> List[dict]:
    """
    è™•ç†å–®å€‹æŸ¥è©¢çš„é é¢æå–

    Args:
        query: æŸ¥è©¢è©
        lang: èªè¨€
        organic_results: SERP æœ‰æ©Ÿçµæœ
        top_n: ä¿ç•™å‰å¹¾å€‹é é¢

    Returns:
        æå–çš„å…§å®¹åˆ—è¡¨
    """
    # ä½¿ç”¨ page_filter ç¯©é¸é«˜å“è³ªé é¢
    filtered = filter_organic_results(organic_results, max_results=top_n)

    print(f"\nğŸ” æŸ¥è©¢ï¼š{query}")
    print(f"   èªè¨€ï¼š{lang}")
    print(f"   åŸå§‹çµæœï¼š{len(organic_results)} å€‹")
    print(f"   ç¯©é¸å¾Œï¼š{len(filtered)} å€‹")

    extracted_pages = []

    for i, result in enumerate(filtered, 1):
        url = result["link"]
        title = result["title"]
        quality_score = result.get("quality_score", 0)

        print(f"\n   [{i}] {title}")
        print(f"       URL: {url}")
        print(f"       å“è³ªåˆ†æ•¸: {quality_score}")

        # ä½¿ç”¨ Tavily Extract æå–å…§å®¹
        content = extract_content_tavily(url)

        if content["success"]:
            content_length = len(content["raw_content"])
            print(f"       âœ… æå–æˆåŠŸï¼ˆ{content_length} å­—ç¬¦ï¼‰")

            extracted_pages.append({
                "query": query,
                "lang": lang,
                "position": i,
                "url": url,
                "title": title,
                "quality_score": quality_score,
                "content": content["raw_content"],
                "extracted_at": content["extracted_at"]
            })
        else:
            print(f"       âŒ æå–å¤±æ•—ï¼š{content.get('error', 'Unknown error')}")

        # API é™æµ
        time.sleep(0.5)

    return extracted_pages

def main():
    # è®€å– SERP åˆ†æçµæœ
    serp_path = config.data_dir / "serp_analysis_bilingual.json"
    if not serp_path.exists():
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{serp_path}ï¼Œè«‹å…ˆåŸ·è¡Œ serp_fetcher_bilingual.py")

    print(f"\nğŸ“‚ è¼‰å…¥ SERP åˆ†æçµæœ...")
    with open(serp_path, "r", encoding="utf-8") as f:
        serp_data = json.load(f)

    serp_results = serp_data["serp_results"]
    print(f"âœ… è¼‰å…¥ {len(serp_results)} å€‹æŸ¥è©¢çš„ SERP çµæœ")

    # å¿«å–
    cache_path = config.data_dir / "cache_extracted_content.json"
    cache = {}
    if cache_path.exists():
        with open(cache_path, "r", encoding="utf-8") as f:
            cache = json.load(f)
        print(f"ğŸ“¦ è¼‰å…¥ {len(cache)} å€‹å¿«å–çš„æå–çµæœ")

    # è™•ç†æ¯å€‹æŸ¥è©¢
    all_extracted_pages = []

    print("\n" + "=" * 60)
    print("ğŸš€ é–‹å§‹æå–é é¢å…§å®¹")
    print("=" * 60)

    for result in tqdm(serp_results, desc="Processing queries"):
        query = result["query"]
        lang = result["lang"]
        organic_results = result["serp_data"]["organic_results"]

        cache_key = f"{query}_{lang}"

        # æª¢æŸ¥å¿«å–
        if cache_key in cache:
            print(f"\nğŸ’¾ ä½¿ç”¨å¿«å–ï¼š{query}")
            extracted = cache[cache_key]
        else:
            # æå–å…§å®¹
            extracted = process_query_pages(query, lang, organic_results, top_n=3)
            cache[cache_key] = extracted

            # å„²å­˜å¿«å–ï¼ˆå³æ™‚æ›´æ–°ï¼‰
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)

        all_extracted_pages.extend(extracted)

    # å„²å­˜æœ€çµ‚çµæœ
    output_path = config.data_dir / "extracted_content_60_pages.json"

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "total_queries": len(serp_results),
            "total_pages_extracted": len(all_extracted_pages),
            "pages": all_extracted_pages
        }, f, ensure_ascii=False, indent=2)

    # çµ±è¨ˆå ±å‘Š
    print("\n" + "=" * 60)
    print("âœ… å…§å®¹æå–å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š{output_path}")
    print(f"ğŸ“ å¿«å–æª”æ¡ˆï¼š{cache_path}")
    print(f"\nğŸ“Š çµ±è¨ˆè³‡è¨Šï¼š")
    print(f"   è™•ç†æŸ¥è©¢æ•¸ï¼š{len(serp_results)}")
    print(f"   æå–é é¢æ•¸ï¼š{len(all_extracted_pages)}")
    print(f"   ç›®æ¨™é é¢æ•¸ï¼š{len(serp_results) * 3}")

    zh_pages = [p for p in all_extracted_pages if p["lang"] == "zh-TW"]
    en_pages = [p for p in all_extracted_pages if p["lang"] == "en"]

    print(f"\n   ç¹é«”ä¸­æ–‡é é¢ï¼š{len(zh_pages)}")
    print(f"   English é é¢ï¼š{len(en_pages)}")

    # è¨ˆç®—å¹³å‡å…§å®¹é•·åº¦
    if all_extracted_pages:
        avg_length = sum(len(p["content"]) for p in all_extracted_pages) / len(all_extracted_pages)
        print(f"\n   å¹³å‡å…§å®¹é•·åº¦ï¼š{avg_length:.0f} å­—ç¬¦")

    print("\n" + "=" * 60)

if __name__ == "__main__":
    main()
