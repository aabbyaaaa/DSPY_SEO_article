# -*- coding: utf-8 -*-
"""
LLM-SEO Pipeline Stage â‘¢: DSPy Analysis Runner (v1.0)
------------------------------------------------------
åŸ·è¡Œä¸‰å€‹ DSPy æ¨¡çµ„ï¼š
1. ContentSummarizer - ç¸½çµç«¶çˆ­è€…å…§å®¹
2. GapAnalyzer - æ‰¾å‡ºå…§å®¹ç¼ºå£
3. OutlineGenerator - ç”Ÿæˆæ–‡ç« å¤§ç¶±

è¼¸å…¥ï¼šdata/serp_analysis.json
è¼¸å‡ºï¼šdata/article_outline.json
"""

import os, json, sys, io
from pathlib import Path
from tqdm import tqdm

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config
from analyze.dspy_modules import (
    ContentSummarizer,
    GapAnalyzer,
    OutlineGenerator,
    init_dspy
)

# ================================================
# åˆå§‹åŒ–
# ================================================
print("\n" + "=" * 60)
print("ğŸš€ DSPy Analysis Module - Stage â‘¢")
print("=" * 60)
print(f"ä¸»é¡Œï¼š{config.topic}")
print(f"DSPy ä¸»æ¨¡å‹ï¼š{config.dspy_model_main}")
print(f"DSPy å°æ¨¡å‹ï¼š{config.dspy_model_small}")

# åˆå§‹åŒ– DSPy
openai_key = config.get_openai_key()
init_dspy(config.dspy_model_main, openai_key)

# åˆå§‹åŒ–ä¸‰å€‹æ¨¡çµ„
print("\nğŸ“¦ åˆå§‹åŒ– DSPy æ¨¡çµ„...")
summarizer = ContentSummarizer()
gap_analyzer = GapAnalyzer()
outline_generator = OutlineGenerator(block_config=config.article_blocks)

print("âœ… æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")

# ================================================
# è¼‰å…¥ SERP æ•¸æ“š
# ================================================
serp_path = config.data_dir / config.output_files["serp_analysis"]
if not serp_path.exists():
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {serp_path}ï¼Œè«‹å…ˆåŸ·è¡Œ analyze/serp_fetcher.py")

print(f"\nğŸ“– è¼‰å…¥ SERP æ•¸æ“šï¼š{serp_path}")
with open(serp_path, "r", encoding="utf-8") as f:
    serp_data = json.load(f)

query_count = serp_data["query_count"]
print(f"âœ… è¼‰å…¥ {query_count} æ¢æŸ¥è©¢çš„ SERP æ•¸æ“š")

# ================================================
# è™•ç†æ¯å€‹æŸ¥è©¢
# ================================================
results = []

print("\n" + "=" * 60)
print("ğŸ”„ é–‹å§‹è™•ç†æŸ¥è©¢...")
print("=" * 60)

for idx, item in enumerate(tqdm(serp_data["serp_results"], desc="DSPy åˆ†æ"), 1):
    query = item["query"]
    serp = item["serp_data"]
    analysis = item["analysis"]

    print(f"\n[{idx}/{query_count}] {query}")

    # ------------------------------------------------
    # Step 1ï¸âƒ£: ContentSummarizer
    # ------------------------------------------------
    print("  ğŸ“ Step 1: ç¸½çµç«¶çˆ­è€…å…§å®¹...")
    organic_results = serp.get("organic_results", [])

    if not organic_results:
        print("  âš ï¸ ç„¡æœ‰æ©Ÿçµæœï¼Œè·³é")
        continue

    try:
        summaries = summarizer.forward(query, organic_results)
        print(f"  âœ… ç¸½çµå®Œæˆï¼š{len(summaries)} å€‹ç«¶çˆ­è€…")
    except Exception as e:
        print(f"  âŒ ContentSummarizer å¤±æ•—ï¼š{e}")
        continue

    # ------------------------------------------------
    # Step 2ï¸âƒ£: GapAnalyzer
    # ------------------------------------------------
    print("  ğŸ” Step 2: åˆ†æå…§å®¹ç¼ºå£...")
    paa_questions = serp.get("people_also_ask", [])
    aiseo_triggered = analysis.get("aiseo_triggered", False)

    try:
        gaps = gap_analyzer.forward(
            query=query,
            competitor_summaries=summaries,
            paa_questions=paa_questions,
            aiseo_triggered=aiseo_triggered
        )
        print(f"  âœ… ç¼ºå£åˆ†æå®Œæˆï¼šæ‰¾åˆ° {len(gaps)} å€‹æ©Ÿæœƒ")

        # é¡¯ç¤ºå‰ 3 å€‹ç¼ºå£
        for i, gap in enumerate(gaps[:3], 1):
            print(f"     {i}. [{gap.gap_type}] {gap.description[:50]}... (åˆ†æ•¸: {gap.opportunity_score:.2f})")

    except Exception as e:
        print(f"  âŒ GapAnalyzer å¤±æ•—ï¼š{e}")
        gaps = []

    # ------------------------------------------------
    # Step 3ï¸âƒ£: OutlineGenerator
    # ------------------------------------------------
    print("  ğŸ“‹ Step 3: ç”Ÿæˆæ–‡ç« å¤§ç¶±...")

    try:
        outline = outline_generator.forward(
            query=query,
            content_gaps=gaps,
            paa_questions=paa_questions,
            aiseo_triggered=aiseo_triggered
        )
        print(f"  âœ… å¤§ç¶±ç”Ÿæˆå®Œæˆ")

        # é¡¯ç¤ºå€å¡Šçµæ§‹
        if "blocks" in outline:
            print(f"     çµæ§‹ï¼š{len(outline['blocks'])} å€‹å€å¡Š")
            for block in outline["blocks"]:
                print(f"       - {block.get('block_name', 'N/A')}: {block.get('word_count_target', 'N/A')} å­—")

    except Exception as e:
        print(f"  âŒ OutlineGenerator å¤±æ•—ï¼š{e}")
        outline = outline_generator._default_outline(query)

    # ------------------------------------------------
    # å„²å­˜çµæœ
    # ------------------------------------------------
    results.append({
        "query": query,
        "aiseo_triggered": aiseo_triggered,
        "competitor_summaries": [s.model_dump() for s in summaries],
        "content_gaps": [g.model_dump() for g in gaps],
        "outline": outline,
        "paa_questions": paa_questions,
        "related_searches": serp.get("related_searches", [])
    })

# ================================================
# è¼¸å‡ºæœ€çµ‚çµæœ
# ================================================
output_path = config.data_dir / "article_outlines.json"

with open(output_path, "w", encoding="utf-8") as f:
    json.dump({
        "topic": config.topic,
        "query_count": len(results),
        "generated_at": str(Path(__file__).stat().st_mtime),
        "outlines": results,
        "summary": {
            "total_gaps_found": sum(len(r["content_gaps"]) for r in results),
            "avg_gaps_per_query": sum(len(r["content_gaps"]) for r in results) / len(results) if results else 0,
            "aiseo_coverage": sum(r["aiseo_triggered"] for r in results) / len(results) if results else 0
        }
    }, f, ensure_ascii=False, indent=2)

print("\n" + "=" * 60)
print("âœ… DSPy åˆ†æå®Œæˆï¼")
print("=" * 60)
print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š{output_path}")
print(f"ğŸ“Š è™•ç†æŸ¥è©¢æ•¸ï¼š{len(results)}")
print(f"ğŸ¯ ç¸½ç¼ºå£æ•¸ï¼š{sum(len(r['content_gaps']) for r in results)}")
print(f"ğŸ“ˆ å¹³å‡ç¼ºå£/æŸ¥è©¢ï¼š{sum(len(r['content_gaps']) for r in results) / len(results):.1f}")
print(f"ğŸ¤– AISEO è¦†è“‹ç‡ï¼š{sum(r['aiseo_triggered'] for r in results) / len(results):.1%}")
print("\nğŸ‰ æº–å‚™é€²å…¥ Stage â‘£ï¼šæ–‡ç« ç”Ÿæˆ")
print("=" * 60 + "\n")
