# -*- coding: utf-8 -*-
"""
LLM-SEO Pipeline Stage â‘¡: Semantic Scoring v2.0 (Bilingual)
-------------------------------------------------------------
æ–°æ¶æ§‹ä¸‰ç¶­è©•åˆ†ç³»çµ±ï¼šAI Coverage + Relevance + SERP Signal

ç‰¹é»ï¼š
1. æ”¯æ´é›™èªï¼ˆç¹é«”ä¸­æ–‡ + è‹±æ–‡ï¼‰
2. AI Coverage: ä½¿ç”¨ Tavily æª¢æ¸¬ AI å¼•æ“å…§å®¹å¯ç”¨æ€§
3. Relevance: ä½¿ç”¨ OpenAI Embeddings è¨ˆç®—ä¸»é¡Œç›¸é—œæ€§ï¼ˆå«è² é¢é—œéµå­—éæ¿¾ï¼‰
4. SERP Signal: æ¨¡æ“¬ PAA + AI Overview + Ads ä¿¡è™Ÿ
5. æ–¹æ¡ˆ B: æ¨™è¨˜ä½†ä¿ç•™æ‰€æœ‰æŸ¥è©¢ï¼ˆquality_tierï¼‰

è¼¸å…¥ï¼š
- data/query_pool_merged.csv
- data/query_vectors_merged.json

è¼¸å‡ºï¼š
- data/semantic_scores.csv
"""

import os, json, sys, io, time
import numpy as np
import pandas as pd
from pathlib import Path
from openai import OpenAI
from tavily import TavilyClient
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config

# ================================================
# åˆå§‹åŒ–
# ================================================
print("\n" + "=" * 60)
print("ğŸ¯ èªç¾©è©•åˆ† v2.0 - é›™èªç‰ˆ")
print("=" * 60)
print(f"ä¸­æ–‡ä¸»é¡Œï¼š{config.topic}")
print(f"è‹±æ–‡ä¸»é¡Œï¼š{config.topic_en}")

# åˆå§‹åŒ– API
tavily = TavilyClient(api_key=config.get_tavily_key())
client = OpenAI(api_key=config.get_openai_key())

# è®€å–é…ç½®
weights = config._config["semantic_scoring"]["weights"]
thresholds = config._config["semantic_scoring"]["thresholds"]
negative_keywords = config._config["semantic_scoring"]["negative_keywords"]

print(f"\nğŸ“Š è©•åˆ†æ¬Šé‡ï¼š")
print(f"   AI Coverage: {weights['ai_coverage']:.0%}")
print(f"   Relevance: {weights['relevance']:.0%}")
print(f"   SERP Signal: {weights['serp_signal']:.0%}")

print(f"\nğŸ“Œ å“è³ªåˆ†ç´šé–¾å€¼ï¼š")
print(f"   High Tier: â‰¥ {thresholds['high_tier']}")
print(f"   Medium Tier: â‰¥ {thresholds['medium_tier']}")
print(f"   Low Tier: < {thresholds['medium_tier']}")

# ================================================
# è¼‰å…¥æŸ¥è©¢æ± èˆ‡å‘é‡
# ================================================
pool_path = config.data_dir / "query_pool.csv"
vec_path = config.data_dir / "query_vectors.json"

if not pool_path.exists():
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {pool_path}ï¼Œè«‹å…ˆåŸ·è¡Œ analyze/stage1_query_generation.py")
if not vec_path.exists():
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° {vec_path}ï¼Œè«‹å…ˆåŸ·è¡Œ analyze/stage1_query_generation.py")

print(f"\nğŸ“‚ è¼‰å…¥æŸ¥è©¢æ± ï¼š{pool_path}")
pool_df = pd.read_csv(pool_path)

print(f"ğŸ“‚ è¼‰å…¥å‘é‡ï¼š{vec_path}")
with open(vec_path, "r", encoding="utf-8") as f:
    vecs = json.load(f)

queries = pool_df["query"].tolist()
vectors = np.array([vecs[q] for q in queries])

# èªè¨€æª¢æ¸¬
languages = []
for q in queries:
    if any('\u4e00' <= c <= '\u9fff' for c in q):
        languages.append("zh-TW")
    else:
        languages.append("en")

zh_count = languages.count("zh-TW")
en_count = languages.count("en")

print(f"\nâœ… è¼‰å…¥ {len(queries)} å€‹æŸ¥è©¢")
print(f"   ç¹é«”ä¸­æ–‡ï¼š{zh_count}")
print(f"   Englishï¼š{en_count}")

# å¿«å–æª”æ¡ˆ
cache_path = config.data_dir / "cache_semantic_v2.json"
cache = {}
if cache_path.exists():
    with open(cache_path, "r", encoding="utf-8") as f:
        cache = json.load(f)
    print(f"\nğŸ“¦ è¼‰å…¥ {len(cache)} å€‹å¿«å–çµæœ")

# ================================================
# Step 1ï¸âƒ£: AI Coverage Score (Tavily)
# ================================================
print("\n" + "=" * 60)
print("ğŸ“¡ Step 1: AI Coverage - Tavily å…§å®¹æª¢æ¸¬")
print("=" * 60)

ai_coverage_scores = []
tavily_max = config._config["semantic_scoring"]["ai_coverage"]["tavily_max_results"]

# ç”¨æ–¼çµ±è¨ˆçš„èª¿è©¦æ•¸æ“š
debug_sources_count = []
debug_avg_scores = []

for q in tqdm(queries, desc="Tavily æœå°‹"):
    cache_key = f"ai_coverage_{q}"
    cache_detail_key = f"ai_coverage_detail_{q}"

    if cache_key in cache and cache_detail_key in cache:
        # å¾å¿«å–è®€å–
        detail = cache[cache_detail_key]
        num_sources = detail["num_sources"]
        avg_tavily_score = detail["avg_score"]

        # ä½¿ç”¨æ–°è©•åˆ†é‚è¼¯é‡æ–°è¨ˆç®—ï¼ˆå³ä½¿æœ‰èˆŠå¿«å–ï¼‰
        quantity_score = min(3, num_sources * 0.3)
        quality_score = avg_tavily_score * 7
        score = round(quantity_score + quality_score, 2)

        debug_sources_count.append(num_sources)
        debug_avg_scores.append(avg_tavily_score)
    else:
        try:
            res = tavily.search(q, max_results=tavily_max)
            results = res.get("results", [])
            num_sources = len(results)

            # èª¿è©¦è¼¸å‡ºï¼šè¨˜éŒ„ä¾†æºæ•¸é‡
            debug_sources_count.append(num_sources)

            # èª¿è©¦è¼¸å‡ºï¼šè¨˜éŒ„ Tavily å“è³ªåˆ†æ•¸
            if num_sources > 0:
                tavily_scores = [r.get("score", 0) for r in results]
                avg_tavily_score = sum(tavily_scores) / num_sources
                debug_avg_scores.append(avg_tavily_score)
            else:
                avg_tavily_score = 0
                debug_avg_scores.append(0)

            # æ–°è©•åˆ†é‚è¼¯ï¼šæ•¸é‡åŸºç¤åˆ† (30%) + å“è³ªåŠ æ¬Šåˆ† (70%)
            # ä¾†æºæ•¸é‡åˆ† (æœ€å¤š 3 åˆ†)
            quantity_score = min(3, num_sources * 0.3)

            # å“è³ªåŠ æ¬Šåˆ† (æœ€å¤š 7 åˆ†)
            quality_score = avg_tavily_score * 7

            # ç¸½åˆ† = æ•¸é‡åˆ† + å“è³ªåˆ†
            score = round(quantity_score + quality_score, 2)

            # ä¿å­˜åˆ†æ•¸å’Œè©³ç´°è³‡è¨Šåˆ°å¿«å–
            cache[cache_key] = score
            cache[cache_detail_key] = {
                "num_sources": num_sources,
                "avg_score": avg_tavily_score,
                "quantity_score": quantity_score,
                "quality_score": quality_score
            }
            time.sleep(0.5)
        except Exception as e:
            print(f"âŒ Tavily error @ {q}: {e}")
            score = 0
            cache[cache_key] = 0
            cache[cache_detail_key] = {"num_sources": 0, "avg_score": 0}
            debug_sources_count.append(0)
            debug_avg_scores.append(0)

    ai_coverage_scores.append(score)

# å³æ™‚å„²å­˜å¿«å–
with open(cache_path, "w", encoding="utf-8") as f:
    json.dump(cache, f, ensure_ascii=False, indent=2)

print(f"âœ… AI Coverage å¹³å‡åˆ†æ•¸ï¼š{np.mean(ai_coverage_scores):.2f}/10")
print(f"   æœ€å°åˆ†æ•¸ï¼š{min(ai_coverage_scores):.2f}/10")
print(f"   æœ€å¤§åˆ†æ•¸ï¼š{max(ai_coverage_scores):.2f}/10")
print(f"   æ¨™æº–å·®ï¼š{np.std(ai_coverage_scores):.2f}")

# èª¿è©¦çµ±è¨ˆå ±å‘Š
if debug_sources_count:
    print(f"\nğŸ” Tavily ä¾†æºæ•¸é‡çµ±è¨ˆï¼š")
    print(f"   æœ€å°å€¼ï¼š{min(debug_sources_count)} å€‹")
    print(f"   æœ€å¤§å€¼ï¼š{max(debug_sources_count)} å€‹")
    print(f"   å¹³å‡å€¼ï¼š{np.mean(debug_sources_count):.2f} å€‹")
    print(f"   ä¸­ä½æ•¸ï¼š{np.median(debug_sources_count):.1f} å€‹")

    # åˆ†å¸ƒçµ±è¨ˆ
    from collections import Counter
    dist = Counter(debug_sources_count)
    print(f"\n   ä¾†æºæ•¸é‡åˆ†å¸ƒï¼š")
    for count in sorted(dist.keys()):
        print(f"      {count} å€‹ä¾†æºï¼š{dist[count]} å€‹æŸ¥è©¢ ({dist[count]/len(debug_sources_count):.1%})")

if debug_avg_scores:
    print(f"\nğŸ” Tavily å“è³ªåˆ†æ•¸çµ±è¨ˆï¼ˆ0-1ï¼‰ï¼š")
    print(f"   æœ€å°å€¼ï¼š{min(debug_avg_scores):.3f}")
    print(f"   æœ€å¤§å€¼ï¼š{max(debug_avg_scores):.3f}")
    print(f"   å¹³å‡å€¼ï¼š{np.mean(debug_avg_scores):.3f}")
    print(f"   ä¸­ä½æ•¸ï¼š{np.median(debug_avg_scores):.3f}")

print(f"\nğŸ“ è©•åˆ†å…¬å¼ï¼šæ•¸é‡åˆ†(30%) + å“è³ªåˆ†(70%)")
print(f"   æ•¸é‡åˆ† = min(3, num_sources * 0.3)")
print(f"   å“è³ªåˆ† = avg_tavily_score * 7")
print(f"   ç¸½åˆ† = æ•¸é‡åˆ† + å“è³ªåˆ†")

# ================================================
# Step 2ï¸âƒ£: Relevance Score (Embedding + è² é¢é—œéµå­—)
# ================================================
print("\n" + "=" * 60)
print("ğŸ§  Step 2: Relevance - Embedding ç›¸é—œæ€§")
print("=" * 60)

# åˆ†èªè¨€è¨ˆç®—ä¸»é¡Œå‘é‡
zh_queries = [q for q, lang in zip(queries, languages) if lang == "zh-TW"]
en_queries = [q for q, lang in zip(queries, languages) if lang == "en"]

zh_vectors = np.array([vecs[q] for q in zh_queries]) if zh_queries else None
en_vectors = np.array([vecs[q] for q in en_queries]) if en_queries else None

# ä¸­æ–‡ä¸»é¡Œå‘é‡
if zh_vectors is not None:
    zh_topic_vec = np.mean(zh_vectors, axis=0)
    print(f"âœ… è¨ˆç®—ä¸­æ–‡ä¸»é¡Œå‘é‡ï¼ˆ{len(zh_queries)} å€‹æŸ¥è©¢ï¼‰")
else:
    zh_topic_vec = None
    print(f"âš ï¸ ç„¡ä¸­æ–‡æŸ¥è©¢")

# è‹±æ–‡ä¸»é¡Œå‘é‡
if en_vectors is not None:
    en_topic_vec = np.mean(en_vectors, axis=0)
    print(f"âœ… è¨ˆç®—è‹±æ–‡ä¸»é¡Œå‘é‡ï¼ˆ{len(en_queries)} å€‹æŸ¥è©¢ï¼‰")
else:
    en_topic_vec = None
    print(f"âš ï¸ ç„¡è‹±æ–‡æŸ¥è©¢")

# è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸
relevance_scores = []
neg_penalty = config._config["semantic_scoring"]["relevance"]["negative_keyword_penalty"]

for q, q_vec, lang in zip(queries, vectors, languages):
    # é¸æ“‡å°æ‡‰èªè¨€çš„ä¸»é¡Œå‘é‡
    if lang == "zh-TW" and zh_topic_vec is not None:
        topic_vec = zh_topic_vec
    elif lang == "en" and en_topic_vec is not None:
        topic_vec = en_topic_vec
    else:
        # é è¨­ä½¿ç”¨å…¨é«”å¹³å‡
        topic_vec = np.mean(vectors, axis=0)

    # è¨ˆç®— cosine similarity
    similarity = cosine_similarity(q_vec.reshape(1, -1), topic_vec.reshape(1, -1))[0][0]

    # åˆ†æ•¸ï¼š0-10 åˆ†
    score = similarity * 10

    # è² é¢é—œéµå­—æ‡²ç½°
    if any(kw in q for kw in negative_keywords):
        score *= neg_penalty
        print(f"âš ï¸ è² é¢é—œéµå­—æ‡²ç½°ï¼š{q[:30]}... ({score:.2f})")

    relevance_scores.append(round(score, 2))

print(f"âœ… Relevance å¹³å‡åˆ†æ•¸ï¼š{np.mean(relevance_scores):.2f}/10")

# ================================================
# Step 3ï¸âƒ£: SERP Signal Score (æ¨¡æ“¬)
# ================================================
print("\n" + "=" * 60)
print("ğŸ” Step 3: SERP Signal - PAA + AI Overview")
print("=" * 60)
print("âš ï¸ æ­¤éšæ®µç‚ºæ¨¡æ“¬åˆ†æ•¸ï¼ˆå¯¦éš› SERP ä¿¡è™Ÿå°‡åœ¨ Stage 3 ç²å–ï¼‰")
print("âš ï¸ æ³¨æ„ï¼šä¸è€ƒæ…® Google Ads ä½œç‚ºè©•åˆ†å› ç´ ")

# æ¨¡æ“¬ SERP Signal åˆ†æ•¸ï¼ˆåŸºæ–¼æŸ¥è©¢ç‰¹å¾µï¼‰
serp_signal_scores = []

for q in queries:
    # ç°¡å–®å•Ÿç™¼å¼è¦å‰‡ï¼ˆå¯¦éš›æ‡‰å¾ SERP API ç²å–ï¼‰
    score = 5.0  # é è¨­åŸºæº–åˆ†

    # é•·å°¾æŸ¥è©¢é€šå¸¸æœ‰æ›´å¤š PAA
    if len(q.split()) >= 5 or len(q) >= 15:
        score += 2.0

    # å•é¡Œå‹æŸ¥è©¢ (æ›´å¯èƒ½è§¸ç™¼ PAA å’Œ AI Overview)
    question_words = ["å¦‚ä½•", "æ€éº¼", "ç‚ºä»€éº¼", "æ˜¯ä»€éº¼", "how", "why", "what", "when"]
    if any(qw in q.lower() for qw in question_words):
        score += 1.5

    # æ•…éšœ/å•é¡Œé¡å‹ï¼ˆé«˜ PAA + AI Overview æ©Ÿç‡ï¼‰
    problem_words = ["æ•…éšœ", "å•é¡Œ", "ç•°å¸¸", "éŒ¯èª¤", "ä¸", "ç„¡æ³•", "error", "issue", "problem", "fail"]
    if any(pw in q.lower() for pw in problem_words):
        score += 1.5

    serp_signal_scores.append(min(10, round(score, 2)))

print(f"âœ… SERP Signal å¹³å‡åˆ†æ•¸ï¼š{np.mean(serp_signal_scores):.2f}/10")

# ================================================
# Step 4ï¸âƒ£: ç¶œåˆè©•åˆ†èˆ‡å“è³ªåˆ†ç´š
# ================================================
print("\n" + "=" * 60)
print("ğŸ“Š Step 4: ç¶œåˆè©•åˆ†èˆ‡å“è³ªåˆ†ç´š")
print("=" * 60)

final_scores = []
quality_tiers = []

for q, ai_cov, rel, serp in zip(queries, ai_coverage_scores, relevance_scores, serp_signal_scores):
    # åŠ æ¬Šè¨ˆç®—
    score = (
        ai_cov * weights["ai_coverage"] +
        rel * weights["relevance"] +
        serp * weights["serp_signal"]
    )
    score = round(score, 2)
    final_scores.append(score)

    # å“è³ªåˆ†ç´š
    if score >= thresholds["high_tier"]:
        tier = "high"
    elif score >= thresholds["medium_tier"]:
        tier = "medium"
    else:
        tier = "low"
    quality_tiers.append(tier)

# ================================================
# Step 5ï¸âƒ£: è¼¸å‡ºçµæœ
# ================================================
out_df = pd.DataFrame({
    "query": queries,
    "lang": languages,
    "ai_coverage_score": ai_coverage_scores,
    "relevance_score": relevance_scores,
    "serp_signal_score": serp_signal_scores,
    "score": final_scores,
    "quality_tier": quality_tiers
})

out_path = config.data_dir / "semantic_scores.csv"
out_df.to_csv(out_path, index=False, encoding="utf-8-sig")

# çµ±è¨ˆå ±å‘Š
high_count = sum(out_df["quality_tier"] == "high")
medium_count = sum(out_df["quality_tier"] == "medium")
low_count = sum(out_df["quality_tier"] == "low")

zh_df = out_df[out_df["lang"] == "zh-TW"]
en_df = out_df[out_df["lang"] == "en"]

print("\n" + "=" * 60)
print("âœ… èªç¾©è©•åˆ†å®Œæˆï¼")
print("=" * 60)
print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š{out_path}")
print(f"\nğŸ“Š æ•´é«”çµ±è¨ˆï¼š")
print(f"   ç¸½æŸ¥è©¢æ•¸ï¼š{len(queries)}")
print(f"   å¹³å‡åˆ†æ•¸ï¼š{np.mean(final_scores):.2f}/10")

print(f"\nğŸ¯ å“è³ªåˆ†ç´šï¼š")
print(f"   âœ… High Tier: {high_count} ({high_count/len(queries):.1%})")
print(f"   âš ï¸ Medium Tier: {medium_count} ({medium_count/len(queries):.1%})")
print(f"   âŒ Low Tier: {low_count} ({low_count/len(queries):.1%})")

print(f"\nğŸŒ èªè¨€åˆ†å¸ƒï¼š")
print(f"   ç¹é«”ä¸­æ–‡ï¼š{len(zh_df)} å€‹ (å¹³å‡åˆ†æ•¸: {zh_df['score'].mean():.2f})")
print(f"   Englishï¼š{len(en_df)} å€‹ (å¹³å‡åˆ†æ•¸: {en_df['score'].mean():.2f})")

print(f"\nğŸ’¡ å¾ŒçºŒè™•ç†å»ºè­°ï¼š")
print(f"   - Stage 3 å°‡åªè™•ç† High + Medium Tier ({high_count + medium_count} å€‹æŸ¥è©¢)")
print(f"   - Low Tier æŸ¥è©¢å·²ä¿ç•™åœ¨æª”æ¡ˆä¸­ä¾›åˆ†æä½¿ç”¨")

print("\nğŸ‰ æº–å‚™é€²å…¥ Stage â‘¢ï¼šSERP åˆ†æ")
print("=" * 60 + "\n")
