# -*- coding: utf-8 -*-
"""
LLM-SEO Pipeline Stage â‘£: Article Generation (v3.0 - 6-Block Structure)
------------------------------------------------------------------------
æ ¹æ“š Stage 3 DSPy åˆ†æçµæœç”Ÿæˆç¬¦åˆ 6-block çµæ§‹çš„ SEO æ–‡ç« 

è¼¸å…¥ï¼š
- data/article_outlines_bilingual.json (æ–‡ç« å¤§ç¶± + content_gaps)
- data/extracted_content_60_pages.json (åƒè€ƒå…§å®¹)
- data/serp_analysis_bilingual.json (SERP åˆ†æ)

è¼¸å‡ºï¼š
- data/final_article.md (æœ€çµ‚ç¶œåˆæ–‡ç« )
- data/final_article_metadata.json (æ–‡ç« å…ƒæ•¸æ“š)
"""

import os
import sys
import io
import json
import re
from pathlib import Path
from typing import List, Dict, Any
from openai import OpenAI

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config, load_config

print("\n" + "=" * 60)
print("ğŸ“ 6-Block æ–‡ç« ç”Ÿæˆæ¨¡çµ„ - Stage â‘£ (v3.0)")
print("=" * 60)
print(f"ä¸»é¡Œï¼š{config.topic} / {config.topic_en}")
print(f"æ–‡ç« çµæ§‹ï¼š{config.article_structure}")

# è¼‰å…¥å®Œæ•´é…ç½®
settings = load_config()
block_config = settings["article"]["blocks"]

# åˆå§‹åŒ– OpenAI
client = OpenAI(api_key=config.get_openai_key())

# ================================================
# è³‡æ–™é©…å‹•çš„ FAQ å•é¡Œé¸æ“‡å‡½æ•¸
# ================================================

def calculate_paa_frequency(outlines_data: Dict) -> List[Dict]:
    """
    è¨ˆç®— PAA å•é¡Œé »ç‡ä¸¦åŠ æ¬Šï¼ˆæ–¹æ¡ˆ C+ å‹•æ…‹é–¾å€¼ï¼‰

    Returns:
        List[Dict]: [{"question": str, "frequency": int, "base_score": float, "source": "paa"}]
    """
    print("\nğŸ“Š è¨ˆç®— PAA å•é¡Œé »ç‡...")

    paa_frequency = {}

    for outline_item in outlines_data["outlines"]:
        for paa in outline_item.get("paa_questions", []):
            q_text = paa.get("question", "") if isinstance(paa, dict) else paa

            if q_text:
                if q_text not in paa_frequency:
                    # è‡ªå‹•æª¢æ¸¬èªè¨€ï¼ˆæ–¹æ¡ˆ Bï¼‰
                    # æª¢æ¸¬æ˜¯å¦åŒ…å«è‹±æ–‡å­—æ¯ï¼ˆæ’é™¤æ¨™é»ç¬¦è™Ÿï¼‰
                    import re
                    is_english = bool(re.search(r'[a-zA-Z]{3,}', q_text))

                    paa_frequency[q_text] = {
                        "question": q_text,
                        "frequency": 0,
                        "source": "paa",
                        "lang": "en" if is_english else "zh-TW"  # è‡ªå‹•æª¢æ¸¬èªè¨€
                    }
                paa_frequency[q_text]["frequency"] += 1

    paa_candidates = list(paa_frequency.values())

    # æ–¹æ¡ˆ C+: è³‡æ–™é©…å‹•çš„å‹•æ…‹é–¾å€¼
    if paa_candidates:
        freq_distribution = [p["frequency"] for p in paa_candidates]
        max_freq = max(freq_distribution)
        median_freq = sorted(freq_distribution)[len(freq_distribution) // 2]

        # å‹•æ…‹è¨ˆç®—é–¾å€¼
        high_threshold = max(3, median_freq + 1)  # è‡³å°‘ 3ï¼Œæˆ–ä¸­ä½æ•¸ + 1
        medium_threshold = max(2, median_freq)    # è‡³å°‘ 2ï¼Œæˆ–ä¸­ä½æ•¸

        high_count = 0
        medium_count = 0
        low_count = 0

        for paa in paa_candidates:
            freq = paa["frequency"]

            # åˆ†å±¤åŠ æ¬Šï¼ˆ15/12/8ï¼‰
            if freq >= high_threshold:
                paa["base_score"] = 15.0  # é«˜é » PAA
                high_count += 1
            elif freq >= medium_threshold:
                paa["base_score"] = 12.0  # ä¸­é » PAA
                medium_count += 1
            else:
                paa["base_score"] = 8.0   # ä½é » PAAï¼ˆä»ä¿ç•™æ©Ÿæœƒï¼‰
                low_count += 1

        print(f"   âœ… æ”¶é›†åˆ° {len(paa_candidates)} å€‹ä¸é‡è¤‡ PAA å•é¡Œ")
        print(f"   ğŸ“ˆ é »ç‡ç¯„åœï¼š{min(freq_distribution)}-{max_freq} æ¬¡ (ä¸­ä½æ•¸: {median_freq})")
        print(f"   ğŸ¯ å‹•æ…‹é–¾å€¼ï¼šé«˜é » â‰¥{high_threshold}, ä¸­é » â‰¥{medium_threshold}")
        print(f"   ğŸ“Š åˆ†å±¤çµ±è¨ˆï¼šé«˜é » {high_count} å€‹ | ä¸­é » {medium_count} å€‹ | ä½é » {low_count} å€‹")
        print(f"   ğŸ† å‰ 3 é«˜é »ï¼š")

        # æŒ‰é »ç‡æ’åºé¡¯ç¤º
        paa_candidates.sort(key=lambda x: x["frequency"], reverse=True)
        for i, paa in enumerate(paa_candidates[:3], 1):
            print(f"      {i}. (é »ç‡ {paa['frequency']}, base_score {paa['base_score']:.1f}) {paa['question'][:50]}...")

    return paa_candidates


def extract_questions_from_content(pages: List[Dict]) -> List[Dict]:
    """
    ä½¿ç”¨è¦å‰‡å¼å¾ extracted content æå–å•é¡Œ

    Returns:
        List[Dict]: [{"question": str, "type": str, "quality_score": float, "source": "extracted", "lang": str}]
    """
    print("\nğŸ” è¦å‰‡å¼æå–å•é¡Œ...")

    # ä¸­æ–‡ pattern
    patterns_zh = [
        (r"(ç‚ºä»€éº¼[^ï¼Ÿ\n]{5,50}ï¼Ÿ)", "why"),
        (r"(å¦‚ä½•[^ï¼Ÿ\n]{5,50}ï¼Ÿ)", "how"),
        (r"(ä»€éº¼æ˜¯[^ï¼Ÿ\n]{5,50}ï¼Ÿ)", "what"),
        (r"([^ï¼Ÿ\n]{5,50}å—ï¼Ÿ)", "yes_no"),
        (r"Q[:ï¼š]\s*([^ï¼Ÿ\n]+ï¼Ÿ)", "faq"),
        (r"å•[:ï¼š]\s*([^ï¼Ÿ\n]+ï¼Ÿ)", "faq"),
    ]

    # è‹±æ–‡ pattern
    patterns_en = [
        (r"(Why [^?\n]{10,80}\?)", "why"),
        (r"(How [^?\n]{10,80}\?)", "how"),
        (r"(What is [^?\n]{10,80}\?)", "what"),
        (r"(Can [^?\n]{10,80}\?)", "yes_no"),
        (r"Q[:]\s*([^?\n]+\?)", "faq"),
    ]

    extracted_questions = []

    for page in pages:
        lang = page.get('lang', 'unknown')
        content = page.get('content', '')
        quality = page.get('quality_score', 5.0)

        # é¸æ“‡å°æ‡‰çš„ pattern
        patterns = patterns_zh if lang == 'zh-TW' else patterns_en

        for pattern, q_type in patterns:
            flags = re.IGNORECASE if lang == 'en' else 0
            matches = re.findall(pattern, content, flags)

            for match in matches:
                question_text = match.strip()
                if 10 < len(question_text) < 100:
                    extracted_questions.append({
                        "question": question_text,
                        "type": q_type,
                        "quality_score": quality,  # ä¿ç•™åŸå§‹ quality_score ä½œç‚ºåƒè€ƒ
                        "source": "extracted",
                        "source_url": page['url'][:60] + "...",
                        "lang": lang,
                        "base_score": 8.0  # æ–¹æ¡ˆ C+ï¼šçµ±ä¸€åŸºç¤åˆ†æ•¸ï¼Œèˆ‡ä½é » PAA ç›¸åŒ
                    })

    # æŒ‰ quality_score æ’åºï¼ˆåƒ…ç”¨æ–¼é¡¯ç¤ºï¼‰
    extracted_questions.sort(key=lambda x: x['quality_score'], reverse=True)

    zh_count = sum(1 for q in extracted_questions if q['lang'] == 'zh-TW')
    en_count = sum(1 for q in extracted_questions if q['lang'] == 'en')
    avg_quality_zh = sum(q['quality_score'] for q in extracted_questions if q['lang'] == 'zh-TW') / zh_count if zh_count > 0 else 0
    avg_quality_en = sum(q['quality_score'] for q in extracted_questions if q['lang'] == 'en') / en_count if en_count > 0 else 0

    print(f"   âœ… æå–åˆ° {len(extracted_questions)} å€‹å•é¡Œï¼ˆçµ±ä¸€ base_score=8.0ï¼‰")
    print(f"   ğŸ“Š èªè¨€åˆ†ä½ˆï¼šä¸­æ–‡ {zh_count} å€‹ (avg quality: {avg_quality_zh:.2f}), è‹±æ–‡ {en_count} å€‹ (avg quality: {avg_quality_en:.2f})")
    print(f"   ğŸ’¡ æ–¹æ¡ˆ C+ï¼šä¸å†ä½¿ç”¨ quality_score æ’åºï¼Œæ”¹ç”¨å¯¦ç”¨æ€§è©•åˆ†")
    print(f"   ğŸ† å‰ 3 å€‹æå–å•é¡Œï¼ˆåƒ…ä¾›åƒè€ƒï¼‰ï¼š")
    for i, q in enumerate(extracted_questions[:3], 1):
        print(f"      {i}. ({q['lang']}, source quality {q['quality_score']:.2f}) {q['question'][:50]}...")

    return extracted_questions


def deduplicate_by_embedding(candidates: List[Dict], threshold: float = 0.85) -> List[Dict]:
    """
    ä½¿ç”¨ OpenAI Embedding å»é‡ï¼ˆè·¨èªè¨€ï¼‰

    Args:
        candidates: å€™é¸å•é¡Œåˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é–¾å€¼ (>= threshold è¦–ç‚ºé‡è¤‡)

    Returns:
        å»é‡å¾Œçš„å•é¡Œåˆ—è¡¨
    """
    print(f"\nğŸ”„ Embedding å»é‡ï¼ˆé–¾å€¼ {threshold}ï¼‰...")

    if not candidates:
        return []

    # å–å¾—æ‰€æœ‰å•é¡Œçš„ embeddings
    questions = [c["question"] for c in candidates]

    try:
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=questions
        )
        embeddings = [item.embedding for item in response.data]
    except Exception as e:
        print(f"   âš ï¸ Embedding API å¤±æ•—ï¼š{e}")
        print(f"   â„¹ï¸ è·³éå»é‡æ­¥é©Ÿ")
        return candidates

    # è¨ˆç®— cosine similarity ä¸¦å»é‡
    import numpy as np

    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    unique_candidates = []
    unique_embeddings = []
    duplicates_removed = 0

    for i, candidate in enumerate(candidates):
        is_duplicate = False

        for j, unique_emb in enumerate(unique_embeddings):
            similarity = cosine_similarity(embeddings[i], unique_emb)

            if similarity >= threshold:
                # ç™¼ç¾é‡è¤‡ï¼Œä¿ç•™ weighted_score æ›´é«˜çš„
                if candidate.get("weighted_score", 0) > unique_candidates[j].get("weighted_score", 0):
                    unique_candidates[j] = candidate
                    unique_embeddings[j] = embeddings[i]

                is_duplicate = True
                duplicates_removed += 1
                break

        if not is_duplicate:
            unique_candidates.append(candidate)
            unique_embeddings.append(embeddings[i])

    print(f"   âœ… å»é‡å‰ï¼š{len(candidates)} å€‹")
    print(f"   âœ… å»é‡å¾Œï¼š{len(unique_candidates)} å€‹")
    print(f"   ğŸ—‘ï¸ ç§»é™¤é‡è¤‡ï¼š{duplicates_removed} å€‹")

    return unique_candidates


def filter_covered_topics(candidates: List[Dict], previous_blocks_text: str, threshold: float = 0.7) -> List[Dict]:
    """
    éæ¿¾å·²åœ¨å‰ 5 å€‹å€å¡Šè¦†è“‹çš„ä¸»é¡Œ

    Args:
        candidates: å€™é¸å•é¡Œåˆ—è¡¨
        previous_blocks_text: å‰ 5 å€‹å€å¡Šçš„æ–‡å­—å…§å®¹
        threshold: ç›¸ä¼¼åº¦é–¾å€¼ (>= threshold è¦–ç‚ºé‡è¤‡ä¸»é¡Œ)

    Returns:
        éæ¿¾å¾Œçš„å•é¡Œåˆ—è¡¨
    """
    print(f"\nğŸš« éæ¿¾é‡è¤‡ä¸»é¡Œï¼ˆé–¾å€¼ {threshold}ï¼‰...")

    if not candidates or not previous_blocks_text:
        return candidates

    # å°‡å‰ 5 å€‹å€å¡Šæ‹†æˆå¥å­
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', previous_blocks_text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

    # å–å¾—æ‰€æœ‰å•é¡Œ + å‰é¢å€å¡Šå¥å­çš„ embeddings
    questions = [c["question"] for c in candidates]
    all_texts = questions + sentences

    try:
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=all_texts
        )
        embeddings = [item.embedding for item in response.data]

        question_embeddings = embeddings[:len(questions)]
        sentence_embeddings = embeddings[len(questions):]
    except Exception as e:
        print(f"   âš ï¸ Embedding API å¤±æ•—ï¼š{e}")
        print(f"   â„¹ï¸ è·³ééæ¿¾æ­¥é©Ÿ")
        return candidates

    # æª¢æŸ¥æ¯å€‹å•é¡Œæ˜¯å¦èˆ‡å‰é¢å€å¡Šé‡è¤‡
    import numpy as np

    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    filtered_candidates = []
    filtered_out = 0

    for i, candidate in enumerate(candidates):
        max_similarity = 0

        for sent_emb in sentence_embeddings:
            similarity = cosine_similarity(question_embeddings[i], sent_emb)
            max_similarity = max(max_similarity, similarity)

        if max_similarity < threshold:
            filtered_candidates.append(candidate)
        else:
            filtered_out += 1
            print(f"   ğŸ—‘ï¸ éæ¿¾ï¼š{candidate['question'][:50]}... (ç›¸ä¼¼åº¦ {max_similarity:.2f})")

    print(f"   âœ… éæ¿¾å‰ï¼š{len(candidates)} å€‹")
    print(f"   âœ… éæ¿¾å¾Œï¼š{len(filtered_candidates)} å€‹")
    print(f"   ğŸ—‘ï¸ ç§»é™¤é‡è¤‡ä¸»é¡Œï¼š{filtered_out} å€‹")

    return filtered_candidates


def translate_question_to_zh_tw(question: str) -> str:
    """
    ä½¿ç”¨ GPT-4o-mini ç¿»è­¯è‹±æ–‡å•é¡Œç‚ºç¹é«”ä¸­æ–‡

    Args:
        question: è‹±æ–‡å•é¡Œ

    Returns:
        ç¹é«”ä¸­æ–‡å•é¡Œ
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯å°ˆæ¥­ç¿»è­¯ï¼Œè«‹å°‡è‹±æ–‡å•é¡Œç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰ã€‚åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜ã€‚"
                },
                {
                    "role": "user",
                    "content": question
                }
            ],
            temperature=0.3,
            max_tokens=100
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"   âš ï¸ ç¿»è­¯å¤±æ•—ï¼š{e}")
        return question  # å¤±æ•—æ™‚è¿”å›åŸæ–‡


def score_question_practicality(question: str, topic: str) -> float:
    """
    ç”¨ GPT-4o-mini å¿«é€Ÿè©•åˆ†å•é¡Œå¯¦ç”¨æ€§ï¼ˆæ–¹æ¡ˆ Aï¼šåš´æ ¼è©•åˆ†ï¼‰

    è©•åˆ†æ¨™æº–ï¼š
    - 10åˆ†ï¼šåŒ…å«å®Œæ•´ç”¢å“åç¨± + ä¸€èˆ¬ä½¿ç”¨è€…å¸¸å• + å•é¡Œå®Œæ•´
    - 8åˆ†ï¼šåŒ…å«ç”¢å“ç›¸é—œè¡“èª + å°ˆæ¥­ä½†å¯¦ç”¨ + å•é¡Œå®Œæ•´
    - 5åˆ†ï¼šå°ˆæ¥­ä¸”æŠ€è¡“ï¼Œä½†å•é¡Œå®Œæ•´
    - 2åˆ†ï¼šå•é¡Œä¸å®Œæ•´æˆ–éæ–¼æ³›åŒ–
    - 1åˆ†ï¼šå®Œå…¨ç„¡é—œ

    Args:
        question: å•é¡Œæ–‡å­—
        topic: ç”¢å“ä¸»é¡Œï¼ˆå¦‚ã€Œé«˜å£“æ»…èŒé‹ã€ï¼‰

    Returns:
        float: 1-10 çš„å¯¦ç”¨æ€§åˆ†æ•¸
    """
    prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å•é¡Œå°æ–¼ã€Œ{topic}ã€ç”¢å“çš„å¯¦ç”¨æ€§ï¼ˆ1-10åˆ†ï¼‰ã€‚

å•é¡Œï¼š{question}

è©•åˆ†æ¨™æº–ï¼ˆåš´æ ¼ç‰ˆï¼‰ï¼š
- 10åˆ†ï¼šåŒ…å«å®Œæ•´ç”¢å“åç¨±ï¼ˆå¦‚ã€Œé«˜å£“æ»…èŒé‹ã€ã€Œautoclaveã€ï¼‰ + ä¸€èˆ¬ä½¿ç”¨è€…å¸¸å•çš„å¯¦ç”¨å•é¡Œï¼ˆå¦‚ã€Œå¦‚ä½•é¸è³¼ã€ã€Œå¦‚ä½•æ¸…æ½”ã€ã€Œå¦‚ä½•æ“ä½œã€ï¼‰ + å•é¡Œèªæ„å®Œæ•´
- 8åˆ†ï¼šåŒ…å«ç”¢å“ç›¸é—œè¡“èª + å°ˆæ¥­ä½†å¯¦ç”¨çš„å•é¡Œï¼ˆå¦‚ã€Œå¦‚ä½•æ ¡æ­£ã€ã€Œæ•…éšœæ’é™¤ã€ã€Œå®šæœŸæª¢æŸ¥ã€ï¼‰ + å•é¡Œèªæ„å®Œæ•´
- 5åˆ†ï¼šå°ˆæ¥­ä¸”æŠ€è¡“çš„å•é¡Œï¼ˆå¦‚ã€ŒF0å€¼ã€ã€ŒSALå€¼ã€ã€ŒPT100æ„Ÿæ¸¬å™¨ã€ï¼‰ï¼Œä½†å•é¡Œèªæ„å®Œæ•´
- 2åˆ†ï¼šå•é¡Œä¸å®Œæ•´ï¼ˆå¦‚ã€Œé«˜å£“æ»…èŒé‡œ å£“åŠ›ï¼Ÿã€ç¼ºå°‘å‹•è©ï¼‰æˆ–éæ–¼æ³›åŒ–ï¼ˆå¦‚ã€Œ60åº¦èƒ½æ®ºèŒå—ï¼Ÿã€æ²’æç”¢å“åç¨±ï¼‰
- 1åˆ†ï¼šå®Œå…¨ç„¡é—œçš„å•é¡Œï¼ˆå¦‚å•å…¶ä»–ç”¢å“ï¼‰

æª¢æŸ¥è¦é»ï¼š
1. å•é¡Œæ˜¯å¦å®Œæ•´ï¼Ÿï¼ˆæœ‰ä¸»è©ã€å‹•è©ã€èªæ„æ¸…æ¥šï¼‰
2. æ˜¯å¦åŒ…å«ç”¢å“åç¨±æˆ–ç›¸é—œè¡“èªï¼Ÿ
3. æ˜¯å¦ç‚ºä¸€èˆ¬ä½¿ç”¨è€…é—œå¿ƒçš„å•é¡Œï¼Ÿ
4. æ˜¯å¦èˆ‡ç”¢å“ç›´æ¥ç›¸é—œï¼Ÿ

åªè¼¸å‡ºæ•¸å­—ï¼ˆ1-10ï¼‰ï¼Œä¸è¦ä»»ä½•èªªæ˜ã€‚"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=5
        )

        score_text = response.choices[0].message.content.strip()
        score = float(score_text)
        return max(1.0, min(10.0, score))
    except Exception as e:
        print(f"   âš ï¸ å¯¦ç”¨æ€§è©•åˆ†å¤±æ•—ï¼ˆ{question[:30]}...ï¼‰ï¼š{e}")
        return 5.0  # å¤±æ•—æ™‚è¿”å›ä¸­ç­‰åˆ†æ•¸


def filter_off_topic_questions(candidates: List[Dict], topic: str, synonyms: List[str]) -> List[Dict]:
    """
    éæ¿¾ä¸åŒ…å«ç”¢å“åç¨±æˆ–åŒç¾©è©çš„å•é¡Œ

    Args:
        candidates: å€™é¸å•é¡Œåˆ—è¡¨
        topic: ä¸»è¦ç”¢å“åç¨±ï¼ˆå¦‚ã€Œé«˜å£“æ»…èŒé‹ã€ï¼‰
        synonyms: åŒç¾©è©åˆ—è¡¨ï¼ˆå¦‚ ["é«˜å£“æ»…èŒé‡œ", "é«˜å£“æ¶ˆæ¯’é‹"]ï¼‰

    Returns:
        éæ¿¾å¾Œçš„å•é¡Œåˆ—è¡¨
    """
    print(f"\nğŸ¯ éæ¿¾ä¸ç›¸é—œå•é¡Œï¼ˆå¿…é ˆåŒ…å«ç”¢å“åç¨±æˆ–åŒç¾©è©ï¼‰...")

    all_keywords = [topic] + synonyms
    filtered_candidates = []
    filtered_out = 0

    for candidate in candidates:
        question = candidate['question'].lower()

        # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•ç”¢å“é—œéµå­—
        has_keyword = any(kw.lower() in question for kw in all_keywords)

        if has_keyword:
            filtered_candidates.append(candidate)
        else:
            filtered_out += 1
            print(f"   âŒ éæ¿¾ï¼š{candidate['question'][:60]}... (ä¸åŒ…å«ç”¢å“åç¨±)")

    print(f"   âœ… éæ¿¾å‰ï¼š{len(candidates)} å€‹")
    print(f"   âœ… éæ¿¾å¾Œï¼š{len(filtered_candidates)} å€‹")
    print(f"   ğŸ—‘ï¸ ç§»é™¤ä¸ç›¸é—œå•é¡Œï¼š{filtered_out} å€‹")

    return filtered_candidates


def normalize_product_name(question: str, topic: str, synonyms: List[str]) -> str:
    """
    å°‡å•é¡Œä¸­çš„åŒç¾©è©çµ±ä¸€æ›¿æ›ç‚ºä¸»è¦ç”¢å“åç¨±

    Args:
        question: å•é¡Œæ–‡å­—
        topic: ä¸»è¦ç”¢å“åç¨±ï¼ˆå¦‚ã€Œé«˜å£“æ»…èŒé‹ã€ï¼‰
        synonyms: åŒç¾©è©åˆ—è¡¨ï¼ˆå¦‚ ["é«˜å£“æ»…èŒé‡œ", "é«˜å£“æ¶ˆæ¯’é‹"]ï¼‰

    Returns:
        æ¨™æº–åŒ–å¾Œçš„å•é¡Œ
    """
    normalized = question
    replaced = False

    for synonym in synonyms:
        if synonym in normalized:
            normalized = normalized.replace(synonym, topic)
            replaced = True
            print(f"   ğŸ”„ æ›¿æ›åŒç¾©è©ï¼š{synonym} â†’ {topic}")

    return normalized


def select_top_10_faq_questions(
    paa_candidates: List[Dict],
    extracted_candidates: List[Dict],
    previous_blocks_text: str
) -> List[str]:
    """
    æ··åˆç­–ç•¥é¸æ“‡æœ€çµ‚ 10 å€‹ FAQ å•é¡Œï¼ˆæ–¹æ¡ˆ C+ å¢å¼·ç‰ˆ + ä¸»é¡Œéæ¿¾ + åŒç¾©è©çµ±ä¸€ï¼‰

    ç­–ç•¥ï¼š
    1. åˆä½µæ‰€æœ‰å€™é¸å•é¡Œ
    2. ğŸ†• éæ¿¾ä¸åŒ…å«ç”¢å“åç¨±çš„å•é¡Œ
    3. LLM å¯¦ç”¨æ€§è©•åˆ†ï¼ˆ1-10 åˆ†ï¼‰
    4. è¨ˆç®—ç¶œåˆåˆ†æ•¸ï¼šfinal_score = base_score + practicality_score * 0.8
    5. Embedding å»é‡ï¼ˆthreshold=0.85ï¼‰
    6. éæ¿¾èˆ‡å‰ 5 å€‹å€å¡Šé‡è¤‡çš„ä¸»é¡Œï¼ˆthreshold=0.7ï¼‰
    7. æŒ‰ final_score æ’åº
    8. é¸æ“‡å‰ 10 å€‹
    9. å¦‚æœæ˜¯è‹±æ–‡å•é¡Œï¼Œç¿»è­¯ç‚ºä¸­æ–‡
    10. ğŸ†• çµ±ä¸€åŒç¾©è©ç‚ºä¸»è¦ç”¢å“åç¨±

    Returns:
        List[str]: 10 å€‹ä¸­æ–‡å•é¡Œï¼ˆå·²æ¨™æº–åŒ–ç”¢å“åç¨±ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ é–‹å§‹è³‡æ–™é©…å‹•çš„ FAQ å•é¡Œé¸æ“‡")
    print("=" * 60)

    # ğŸ†• å®šç¾©ç”¢å“åç¨±èˆ‡åŒç¾©è©ï¼ˆå¾ settings.yaml çš„ base_seeds_zh è®€å–ï¼‰
    topic = config.topic  # "é«˜å£“æ»…èŒé‹"ï¼ˆä¸»è¦åç¨±ï¼‰
    all_seeds = settings["query_generation"]["base_seeds_zh"]
    synonyms = [s for s in all_seeds if s != topic]  # åŒç¾©è©ï¼ˆæ’é™¤ä¸»è¦åç¨±ï¼‰

    print(f"\nğŸ·ï¸ ç”¢å“åç¨±è¨­å®šï¼š")
    print(f"   ä¸»è¦åç¨±ï¼š{topic}")
    print(f"   åŒç¾©è©ï¼š{', '.join(synonyms)}")

    # 1. åˆä½µæ‰€æœ‰å€™é¸å•é¡Œ
    all_candidates = paa_candidates + extracted_candidates
    print(f"\nğŸ“¦ å€™é¸å•é¡Œæ± ï¼š{len(all_candidates)} å€‹")
    print(f"   - PAAï¼š{len(paa_candidates)} å€‹")
    print(f"   - Extractedï¼š{len(extracted_candidates)} å€‹")

    # ğŸ†• 2. éæ¿¾ä¸åŒ…å«ç”¢å“åç¨±çš„å•é¡Œ
    on_topic_candidates = filter_off_topic_questions(all_candidates, topic, synonyms)

    # 3. LLM å¯¦ç”¨æ€§è©•åˆ†ï¼ˆæ–¹æ¡ˆ C+ æ ¸å¿ƒï¼‰
    print(f"\nğŸ¤– é–‹å§‹ LLM å¯¦ç”¨æ€§è©•åˆ†ï¼ˆä½¿ç”¨ gpt-4o-miniï¼‰...")
    for i, candidate in enumerate(on_topic_candidates, 1):
        practicality_score = score_question_practicality(candidate['question'], config.topic)
        candidate['practicality_score'] = practicality_score

        # è¨ˆç®—ç¶œåˆåˆ†æ•¸ï¼šfinal_score = base_score + practicality_score * 0.8
        base_score = candidate.get('base_score', 0)
        candidate['final_score'] = base_score + practicality_score * 0.8

        if i <= 5 or i % 20 == 0:  # é¡¯ç¤ºå‰ 5 å€‹ + æ¯ 20 å€‹é¡¯ç¤ºé€²åº¦
            print(f"   [{i}/{len(on_topic_candidates)}] å¯¦ç”¨æ€§ {practicality_score:.1f} | åŸºç¤ {base_score:.1f} | ç¶œåˆ {candidate['final_score']:.2f} | {candidate['question'][:40]}...")

    print(f"âœ… å®Œæˆ {len(on_topic_candidates)} å€‹å•é¡Œçš„å¯¦ç”¨æ€§è©•åˆ†")

    # 4. Embedding å»é‡
    unique_candidates = deduplicate_by_embedding(on_topic_candidates, threshold=0.85)

    # 5. éæ¿¾é‡è¤‡ä¸»é¡Œï¼ˆèˆ‡å‰ 5 å€‹å€å¡Šï¼‰
    filtered_candidates = filter_covered_topics(unique_candidates, previous_blocks_text, threshold=0.7)

    # 6. æŒ‰ final_score æ’åºï¼ˆæ–¹æ¡ˆ C+ ç¶œåˆè©•åˆ†ï¼‰
    filtered_candidates.sort(key=lambda x: x.get("final_score", 0), reverse=True)

    # 7. é¸æ“‡å‰ 10 å€‹
    top_10_candidates = filtered_candidates[:10]

    print(f"\nğŸ† æœ€çµ‚é¸æ“‡å‰ 10 å€‹å•é¡Œï¼ˆæ–¹æ¡ˆ C+ ç¶œåˆè©•åˆ†ï¼‰ï¼š")
    paa_count = sum(1 for c in top_10_candidates if c['source'] == 'paa')
    extracted_count = sum(1 for c in top_10_candidates if c['source'] == 'extracted')
    print(f"   ğŸ“Š ä¾†æºåˆ†ä½ˆï¼šPAA {paa_count} å€‹ | Extracted {extracted_count} å€‹")

    for i, candidate in enumerate(top_10_candidates, 1):
        print(f"   {i}. [{candidate['source']}] ç¶œåˆåˆ†æ•¸ {candidate.get('final_score', 0):.2f} (åŸºç¤ {candidate.get('base_score', 0):.1f} + å¯¦ç”¨æ€§ {candidate.get('practicality_score', 0):.1f}Ã—0.8)")
        print(f"      {candidate['question'][:80]}...")

    # 8. ç¿»è­¯è‹±æ–‡å•é¡Œ
    final_questions = []
    for candidate in top_10_candidates:
        if candidate['lang'] == 'en':
            print(f"\nğŸŒ ç¿»è­¯è‹±æ–‡å•é¡Œï¼š{candidate['question'][:50]}...")
            zh_question = translate_question_to_zh_tw(candidate['question'])
            print(f"   â¡ï¸ {zh_question}")
            final_questions.append(zh_question)
        else:
            final_questions.append(candidate['question'])

    # ğŸ†• 9. çµ±ä¸€åŒç¾©è©
    print(f"\nğŸ”„ çµ±ä¸€ç”¢å“åç¨±åŒç¾©è©...")
    normalized_questions = []
    for question in final_questions:
        normalized = normalize_product_name(question, topic, synonyms)
        normalized_questions.append(normalized)

    print("\nâœ… FAQ å•é¡Œé¸æ“‡å®Œæˆï¼")
    print("=" * 60)

    return normalized_questions

# ================================================
# è¼‰å…¥æ‰€æœ‰è³‡æ–™
# ================================================

print("\nğŸ“‚ è¼‰å…¥è³‡æ–™...")

# 1. æ–‡ç« å¤§ç¶± (åŒ…å« content_gaps)
outlines_path = config.data_dir / "article_outlines_bilingual.json"
with open(outlines_path, "r", encoding="utf-8") as f:
    outlines_data = json.load(f)

# 2. æå–çš„å…§å®¹ (cache_extracted_content.json - 114 pages with quality_score)
content_path = config.data_dir / "cache_extracted_content.json"
with open(content_path, "r", encoding="utf-8") as f:
    content_data_raw = json.load(f)

# å°‡ content_data è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼ï¼ˆä¸­è‹±æ–‡åˆä½µï¼‰
all_pages = []
for lang_key, pages_list in content_data_raw.items():
    if isinstance(pages_list, list):
        all_pages.extend(pages_list)

# æŒ‰ quality_score æ’åº
all_pages.sort(key=lambda x: x.get('quality_score', 0), reverse=True)

# 3. SERP åˆ†æ
serp_path = config.data_dir / "serp_analysis_bilingual.json"
with open(serp_path, "r", encoding="utf-8") as f:
    serp_data = json.load(f)

print(f"âœ… è¼‰å…¥ {len(outlines_data['outlines'])} å€‹æŸ¥è©¢å¤§ç¶±")
print(f"âœ… è¼‰å…¥ {len(all_pages)} å€‹åƒè€ƒé é¢ï¼ˆä¸­æ–‡ {sum(1 for p in all_pages if p.get('lang')=='zh-TW')} å€‹ + è‹±æ–‡ {sum(1 for p in all_pages if p.get('lang')=='en')} å€‹ï¼‰")
print(f"âœ… è¼‰å…¥ {len(serp_data['serp_results'])} å€‹ SERP åˆ†æ")

# ================================================
# æ•´åˆè³‡æ–™
# ================================================

print("\nğŸ”— æ•´åˆè³‡æ–™...")

# æº–å‚™åƒè€ƒå…§å®¹æ‘˜è¦ï¼ˆä½¿ç”¨å“è³ªæœ€é«˜çš„é é¢ï¼‰
reference_summaries = []
for page in all_pages[:20]:  # å–å‰ 20 å€‹é«˜å“è³ªé é¢
    content_preview = page.get("content", "")[:1000]
    reference_summaries.append({
        "title": page.get("title", ""),
        "url": page.get("url", ""),
        "preview": content_preview,
        "query": page.get("query", ""),
        "quality_score": page.get("quality_score", 0),
        "lang": page.get("lang", "unknown")
    })

# æ”¶é›†æ‰€æœ‰ content_gaps (æŒ‰ target_block åˆ†çµ„)
gaps_by_block = {
    "quick_summary": [],
    "definition": [],
    "uses": [],
    "buying_guide": [],
    "maintenance": [],
    "faq": []
}

for outline_item in outlines_data["outlines"]:
    for gap in outline_item.get("content_gaps", []):
        target_block = gap.get("target_block", "")
        if target_block in gaps_by_block:
            gaps_by_block[target_block].append({
                "query": outline_item["query"],
                "gap_type": gap["gap_type"],
                "description": gap["description"],
                "opportunity_score": gap["opportunity_score"],
                "recommended_action": gap["recommended_action"]
            })

print(f"âœ… æº–å‚™ {len(reference_summaries)} å€‹åƒè€ƒå…§å®¹ï¼ˆæŒ‰ quality_score æ’åºï¼‰")
print(f"   å“è³ªåˆ†ä½ˆï¼š{min([r['quality_score'] for r in reference_summaries]):.2f} - {max([r['quality_score'] for r in reference_summaries]):.2f}")
print(f"   èªè¨€åˆ†ä½ˆï¼šä¸­æ–‡ {sum(1 for r in reference_summaries if r['lang']=='zh-TW')} å€‹ + è‹±æ–‡ {sum(1 for r in reference_summaries if r['lang']=='en')} å€‹")
print(f"âœ… Content Gaps åˆ†ä½ˆï¼š")
for block, gaps in gaps_by_block.items():
    print(f"   - {block}: {len(gaps)} å€‹ç¼ºå£")

# ================================================
# ğŸ¯ è³‡æ–™é©…å‹•çš„ FAQ å•é¡Œé¸æ“‡
# ================================================

print("\n" + "=" * 60)
print("ğŸ¯ é–‹å§‹è³‡æ–™é©…å‹•çš„ FAQ å•é¡Œé¸æ“‡")
print("=" * 60)

# 1. è¨ˆç®— PAA é »ç‡
paa_candidates = calculate_paa_frequency(outlines_data)

# 2. è¦å‰‡å¼æå–å•é¡Œ
extracted_candidates = extract_questions_from_content(all_pages)

# ================================================
# ç”Ÿæˆå‡½æ•¸ - 6 Blocks
# ================================================

def generate_quick_summary(topic: str, topic_en: str, references: List[Dict], gaps: List[Dict]) -> str:
    """ç”Ÿæˆ Quick Summary (40-50å­—ï¼Œ2-3å¥)"""

    cfg = block_config["quick_summary"]
    ref_text = "\n".join([f"- {r['title']}: {r['preview'][:200]}..." for r in references[:3]])
    gaps_text = "\n".join([f"- {g['description']}" for g in gaps[:3]])
    avoid_text = "ã€".join(cfg["avoid"])

    prompt = f"""è«‹ç‚ºã€Œ{topic}ã€(è‹±æ–‡: {topic_en}) æ’°å¯« Quick Summaryã€‚

**å­—æ•¸è¦æ±‚ï¼š{cfg['word_count_min']}-{cfg['word_count_max']} å­—**
**å¥æ•¸ï¼š{cfg['sentences']} å¥è©±**
**å¿…é ˆåŒ…å«ï¼š{", ".join(cfg['must_include'])}**

**åš´æ ¼é¿å…ï¼š**
{avoid_text}

**å…§å®¹ç¼ºå£æ©Ÿæœƒï¼ˆè«‹é‡å°é€™äº›å„ªåŒ–ï¼‰ï¼š**
{gaps_text}

**åƒè€ƒè³‡æ–™ï¼š**
{ref_text}

è«‹ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸è¦åŠ æ¨™é¡Œæˆ–èªªæ˜ã€‚"""

    response = client.chat.completions.create(
        model=config.dspy_model_main,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=200
    )

    return response.choices[0].message.content.strip()


def generate_definition(topic: str, topic_en: str, references: List[Dict], gaps: List[Dict], quick_summary: str) -> str:
    """ç”Ÿæˆ Definition (100-150å­—)"""

    cfg = block_config["definition"]
    ref_text = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['preview'][:500]}..." for r in references[:8]])
    gaps_text = "\n".join([f"- {g['description']}" for g in gaps[:3]])
    avoid_text = "ã€".join(cfg["avoid"])

    prompt = f"""è«‹ç‚ºã€Œ{topic}ã€(è‹±æ–‡: {topic_en}) æ’°å¯« Definition å€å¡Šã€‚

**å­—æ•¸è¦æ±‚ï¼š{cfg['word_count_min']}-{cfg['word_count_max']} å­—**
**å¿…é ˆåŒ…å«ï¼š{", ".join(cfg['must_include'])}**

**åš´æ ¼é¿å…ï¼š**
{avoid_text}

**å·²åœ¨ Quick Summary èªªéçš„å…§å®¹ï¼ˆä¸è¦é‡è¤‡ï¼‰ï¼š**
{quick_summary}

**å…§å®¹ç¼ºå£æ©Ÿæœƒï¼ˆè«‹é‡å°é€™äº›å„ªåŒ–ï¼‰ï¼š**
{gaps_text}

**åƒè€ƒè³‡æ–™ï¼š**
{ref_text}

è«‹ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸è¦åŠ æ¨™é¡Œæˆ–èªªæ˜ã€‚"""

    response = client.chat.completions.create(
        model=config.dspy_model_main,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=400
    )

    return response.choices[0].message.content.strip()


def generate_uses(topic: str, topic_en: str, references: List[Dict], gaps: List[Dict], definition: str) -> str:
    """ç”Ÿæˆ Uses (100-150å­—ï¼Œåƒ…æ‡‰ç”¨å ´æ™¯)"""

    cfg = block_config["uses"]
    ref_text = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['preview'][:800]}..." for r in references[:10]])
    gaps_text = "\n".join([f"- {g['description']}" for g in gaps[:3]])
    avoid_text = "ã€".join(cfg["avoid"])

    prompt = f"""è«‹ç‚ºã€Œ{topic}ã€(è‹±æ–‡: {topic_en}) æ’°å¯« Usesï¼ˆæ‡‰ç”¨å ´æ™¯ï¼‰å€å¡Šã€‚

**å­—æ•¸è¦æ±‚ï¼š{cfg['word_count_min']}-{cfg['word_count_max']} å­—**
**å ´æ™¯æ•¸é‡ï¼š{cfg['scenarios_min']}-{cfg['scenarios_max']} å€‹**
**æ ¼å¼è¦æ±‚ï¼š{cfg['structure']}**
**å¿…é ˆåŒ…å«ï¼š{", ".join(cfg['must_include'])}**

**ğŸš¨ åš´æ ¼é¿å…ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š**
{avoid_text}

**ç‰¹åˆ¥æ³¨æ„ï¼šåƒ…æè¿°ã€Œæ‡‰ç”¨å ´æ™¯ã€ï¼Œä¸è¦åŒ…å«ã€Œæ“ä½œæ­¥é©Ÿã€ã€ã€Œä½¿ç”¨æ–¹æ³•ã€ã€ã€Œæ³¨æ„äº‹é …ã€ï¼**

**å·²åœ¨ Definition èªªéçš„å…§å®¹ï¼ˆä¸è¦é‡è¤‡ï¼‰ï¼š**
{definition[:200]}...

**å…§å®¹ç¼ºå£æ©Ÿæœƒï¼ˆè«‹é‡å°é€™äº›å„ªåŒ–ï¼‰ï¼š**
{gaps_text}

**åƒè€ƒè³‡æ–™ï¼š**
{ref_text}

è«‹ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸è¦åŠ æ¨™é¡Œæˆ–èªªæ˜ã€‚ä½¿ç”¨æ®µè½å¼æè¿°ï¼Œä»¥å†’è™Ÿåˆ†éš”å ´æ™¯åç¨±å’Œæè¿°ã€‚"""

    response = client.chat.completions.create(
        model=config.dspy_model_main,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=400
    )

    return response.choices[0].message.content.strip()


def generate_buying_guide(topic: str, topic_en: str, references: List[Dict], gaps: List[Dict], previous_content: str) -> str:
    """ç”Ÿæˆ Buying Guide (50-80å­—å¿«é€Ÿé‡é» + 250å­—è©³ç´°)"""

    cfg = block_config["buying_guide"]
    ref_text = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['preview']}..." for r in references[:12]])
    gaps_text = "\n".join([f"- {g['description']}" for g in gaps[:5]])
    avoid_text = "ã€".join(cfg["avoid"])

    prompt = f"""è«‹ç‚ºã€Œ{topic}ã€(è‹±æ–‡: {topic_en}) æ’°å¯« Buying Guideï¼ˆé¸è³¼æŒ‡å—ï¼‰å€å¡Šã€‚

**æ ¼å¼è¦æ±‚ï¼š{cfg['format']}**
**å¿«é€Ÿé‡é»å­—æ•¸ï¼š{cfg['quick_summary_words']} å­—**
**è©³ç´°å…§å®¹å­—æ•¸ï¼š{cfg['detailed_words']} å­—**
**å¿…é ˆåŒ…å«ï¼š{", ".join(cfg['must_include'])}**

**ğŸš¨ åš´æ ¼é¿å…ï¼š**
{avoid_text}

**é‡è¦æ ¼å¼è¦å‰‡ï¼š**
1. å¿…é ˆä»¥ã€Œâ–¸ å¿«é€Ÿé‡é»ï¼šã€é–‹é ­
2. ä½¿ç”¨ã€Œï½œã€åˆ†éš”å¿«é€Ÿé‡é»å’Œè©³ç´°å…§å®¹
3. **ä¸èƒ½æ›è¡Œ** - æ•´å€‹å…§å®¹å¿…é ˆåœ¨ä¸€è¡Œå…§
4. ä¸è¦ä½¿ç”¨æ¢åˆ—å¼ï¼ˆ1.2.3.æˆ–-ï¼‰

**å‰é¢å·²èªªéçš„å…§å®¹ï¼ˆä¸è¦é‡è¤‡ï¼‰ï¼š**
{previous_content[:300]}...

**å…§å®¹ç¼ºå£æ©Ÿæœƒï¼ˆè«‹é‡å°é€™äº›å„ªåŒ–ï¼‰ï¼š**
{gaps_text}

**åƒè€ƒè³‡æ–™ï¼š**
{ref_text}

è«‹ç›´æ¥è¼¸å‡ºä¸€æ®µä¸æ›è¡Œçš„æ–‡å­—ï¼Œæ ¼å¼ç‚ºï¼šâ–¸ å¿«é€Ÿé‡é»ï¼š[50-80å­—ç°¡ç­”]ï½œ [250å­—è©³ç´°å…§å®¹]"""

    response = client.chat.completions.create(
        model=config.dspy_model_main,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=600
    )

    content = response.choices[0].message.content.strip()
    # ç¢ºä¿ä¸æ›è¡Œ
    content = content.replace('\n', ' ').replace('\r', '')
    return content


def generate_maintenance(topic: str, topic_en: str, references: List[Dict], gaps: List[Dict], previous_content: str) -> str:
    """ç”Ÿæˆ Maintenance (50-80å­—å¿«é€Ÿé‡é» + 250å­—è©³ç´°)"""

    cfg = block_config["maintenance"]
    ref_text = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['preview']}..." for r in references[:12]])
    gaps_text = "\n".join([f"- {g['description']}" for g in gaps[:5]])
    avoid_text = "ã€".join(cfg["avoid"])

    prompt = f"""è«‹ç‚ºã€Œ{topic}ã€(è‹±æ–‡: {topic_en}) æ’°å¯« Maintenanceï¼ˆä¿é¤Šç¶­è­·ï¼‰å€å¡Šã€‚

**æ ¼å¼è¦æ±‚ï¼š{cfg['format']}**
**å¿«é€Ÿé‡é»å­—æ•¸ï¼š{cfg['quick_summary_words']} å­—**
**è©³ç´°å…§å®¹å­—æ•¸ï¼š{cfg['detailed_words']} å­—**
**å¿…é ˆåŒ…å«ï¼š{", ".join(cfg['must_include'])}**

**ğŸš¨ åš´æ ¼é¿å…ï¼š**
{avoid_text}

**é‡è¦æ ¼å¼è¦å‰‡ï¼š**
1. å¿…é ˆä»¥ã€Œâ–¸ å¿«é€Ÿé‡é»ï¼šã€é–‹é ­
2. ä½¿ç”¨ã€Œï½œã€åˆ†éš”å¿«é€Ÿé‡é»å’Œè©³ç´°å…§å®¹
3. **ä¸èƒ½æ›è¡Œ** - æ•´å€‹å…§å®¹å¿…é ˆåœ¨ä¸€è¡Œå…§
4. ä¸è¦ä½¿ç”¨æ¢åˆ—å¼ï¼ˆ1.2.3.æˆ–-ï¼‰

**å‰é¢å·²èªªéçš„å…§å®¹ï¼ˆä¸è¦é‡è¤‡ï¼‰ï¼š**
{previous_content[:300]}...

**å…§å®¹ç¼ºå£æ©Ÿæœƒï¼ˆè«‹é‡å°é€™äº›å„ªåŒ–ï¼‰ï¼š**
{gaps_text}

**åƒè€ƒè³‡æ–™ï¼š**
{ref_text}

è«‹ç›´æ¥è¼¸å‡ºä¸€æ®µä¸æ›è¡Œçš„æ–‡å­—ï¼Œæ ¼å¼ç‚ºï¼šâ–¸ å¿«é€Ÿé‡é»ï¼š[50-80å­—ç°¡ç­”]ï½œ [250å­—è©³ç´°å…§å®¹]"""

    response = client.chat.completions.create(
        model=config.dspy_model_main,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=600
    )

    content = response.choices[0].message.content.strip()
    # ç¢ºä¿ä¸æ›è¡Œ
    content = content.replace('\n', ' ').replace('\r', '')
    return content


def generate_faq(topic: str, topic_en: str, references: List[Dict], selected_questions: List[str], gaps: List[Dict], all_previous_content: str) -> Dict:
    """ç”Ÿæˆ FAQ (1200-3000å­—ï¼Œ10å€‹å•é¡Œ)

    Args:
        selected_questions: å·²é€éè³‡æ–™é©…å‹•æ–¹å¼é¸æ“‡çš„ 10 å€‹å•é¡Œï¼ˆä¸­æ–‡ï¼‰
    """

    cfg = block_config["faq"]
    ref_text = "\n\n".join([f"ã€{r['title']}ã€‘\n{r['preview']}" for r in references])
    questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(selected_questions)])
    gaps_text = "\n".join([f"- {g['description']}" for g in gaps[:5]])
    avoid_text = "ã€".join(cfg["avoid"])

    prompt = f"""è«‹ç‚ºã€Œ{topic}ã€(è‹±æ–‡: {topic_en}) æ’°å¯« FAQ å€å¡Šã€‚

**é‡è¦ï¼šä»¥ä¸‹æ˜¯å·²é€éè³‡æ–™é©…å‹•æ–¹å¼é¸æ“‡çš„ {len(selected_questions)} å€‹å•é¡Œï¼Œè«‹ç›´æ¥é‡å°é€™äº›å•é¡Œæ’°å¯«å›ç­”ã€‚**

**å¿…é ˆå›ç­”çš„å•é¡Œï¼ˆå…± {len(selected_questions)} å€‹ï¼‰ï¼š**
{questions_text}

**æ¯å€‹å›ç­”ï¼šä¸è¶…é {cfg['answer_max_words']} å­—**
**ç¸½å­—æ•¸ï¼š{cfg['word_count_min']}-{cfg['word_count_max']} å­—**
**å•é¡Œé¡å‹åƒè€ƒï¼š{", ".join(cfg['content_types'])}**

**ğŸš¨ åš´æ ¼é¿å…ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š**
{avoid_text}

**ç‰¹åˆ¥é‡è¦ï¼šä¸è¦é‡è¤‡å‰é¢ Quick Summaryã€Definitionã€Usesã€Buying Guideã€Maintenance å€å¡Šå·²èªªéçš„å…§å®¹ï¼**

**å‰é¢æ‰€æœ‰å€å¡Šå·²èªªéçš„å…§å®¹ï¼ˆçµ•å°ä¸è¦é‡è¤‡ï¼‰ï¼š**
{all_previous_content[:500]}...

**å…§å®¹ç¼ºå£æ©Ÿæœƒï¼ˆè«‹é‡å°é€™äº›å„ªåŒ–å›ç­”ï¼‰ï¼š**
{gaps_text}

**åƒè€ƒè³‡æ–™ï¼š**
{ref_text}

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "faqs": [
    {{
      "question": "å•é¡Œ1ï¼ˆå¿…é ˆèˆ‡ä¸Šé¢åˆ—å‡ºçš„å•é¡Œå®Œå…¨ä¸€è‡´ï¼‰",
      "answer": "å›ç­”1"
    }},
    {{
      "question": "å•é¡Œ2ï¼ˆå¿…é ˆèˆ‡ä¸Šé¢åˆ—å‡ºçš„å•é¡Œå®Œå…¨ä¸€è‡´ï¼‰",
      "answer": "å›ç­”2"
    }},
    ...ï¼ˆå…± {len(selected_questions)} å€‹å•ç­”ï¼‰
  ]
}}

ç¢ºä¿ï¼š
1. å•é¡Œæ–‡å­—èˆ‡ä¸Šé¢åˆ—å‡ºçš„å®Œå…¨ä¸€è‡´
2. æ¯å€‹å›ç­”éƒ½æ˜¯å…¨æ–°çš„å…§å®¹ï¼Œä¸é‡è¤‡å‰é¢ä»»ä½•å€å¡Š
3. å›ç­”è©³ç´°ä¸”å¯¦ç”¨ï¼Œä¸è¦æ³›æ³›è€Œè«‡"""

    response = client.chat.completions.create(
        model=config.dspy_model_main,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=5000
    )

    response_text = response.choices[0].message.content.strip()

    # æå– JSON
    try:
        faq_data = json.loads(response_text)
    except:
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            faq_data = json.loads(json_match.group())
        else:
            faq_data = {"faqs": []}

    return faq_data


# ================================================
# ç”Ÿæˆæ–‡ç« 
# ================================================

print("\n" + "=" * 60)
print("ğŸš€ é–‹å§‹ç”Ÿæˆ 6-Block æ–‡ç« ")
print("=" * 60)

generated_blocks = {}

# 1ï¸âƒ£ Quick Summary
print("\nâ³ ç”Ÿæˆ Quick Summary...")
generated_blocks["quick_summary"] = generate_quick_summary(
    config.topic,
    config.topic_en,
    reference_summaries,
    gaps_by_block["quick_summary"]
)
print(f"âœ… Quick Summary å®Œæˆ ({len(generated_blocks['quick_summary'])} å­—)")

# 2ï¸âƒ£ Definition
print("\nâ³ ç”Ÿæˆ Definition...")
generated_blocks["definition"] = generate_definition(
    config.topic,
    config.topic_en,
    reference_summaries,
    gaps_by_block["definition"],
    generated_blocks["quick_summary"]
)
print(f"âœ… Definition å®Œæˆ ({len(generated_blocks['definition'])} å­—)")

# 3ï¸âƒ£ Uses
print("\nâ³ ç”Ÿæˆ Uses...")
generated_blocks["uses"] = generate_uses(
    config.topic,
    config.topic_en,
    reference_summaries,
    gaps_by_block["uses"],
    generated_blocks["definition"]
)
print(f"âœ… Uses å®Œæˆ ({len(generated_blocks['uses'])} å­—)")

# 4ï¸âƒ£ Buying Guide
print("\nâ³ ç”Ÿæˆ Buying Guide...")
previous_content = generated_blocks["quick_summary"] + generated_blocks["definition"] + generated_blocks["uses"]
generated_blocks["buying_guide"] = generate_buying_guide(
    config.topic,
    config.topic_en,
    reference_summaries,
    gaps_by_block["buying_guide"],
    previous_content
)
print(f"âœ… Buying Guide å®Œæˆ ({len(generated_blocks['buying_guide'])} å­—)")

# 5ï¸âƒ£ Maintenance
print("\nâ³ ç”Ÿæˆ Maintenance...")
previous_content += generated_blocks["buying_guide"]
generated_blocks["maintenance"] = generate_maintenance(
    config.topic,
    config.topic_en,
    reference_summaries,
    gaps_by_block["maintenance"],
    previous_content
)
print(f"âœ… Maintenance å®Œæˆ ({len(generated_blocks['maintenance'])} å­—)")

# ğŸ¯ é¸æ“‡ FAQ å•é¡Œï¼ˆè³‡æ–™é©…å‹•ï¼‰
print("\nâ³ é¸æ“‡ FAQ å•é¡Œï¼ˆè³‡æ–™é©…å‹•ï¼‰...")
all_previous_content = "\n\n".join([
    f"ã€Quick Summaryã€‘{generated_blocks['quick_summary']}",
    f"ã€Definitionã€‘{generated_blocks['definition']}",
    f"ã€Usesã€‘{generated_blocks['uses']}",
    f"ã€Buying Guideã€‘{generated_blocks['buying_guide']}",
    f"ã€Maintenanceã€‘{generated_blocks['maintenance']}"
])

# å‘¼å«è³‡æ–™é©…å‹•çš„ FAQ å•é¡Œé¸æ“‡å‡½æ•¸
selected_faq_questions = select_top_10_faq_questions(
    paa_candidates,
    extracted_candidates,
    all_previous_content
)

# 6ï¸âƒ£ FAQ
print("\nâ³ ç”Ÿæˆ FAQ...")
faq_data = generate_faq(
    config.topic,
    config.topic_en,
    reference_summaries,
    selected_faq_questions,  # ä½¿ç”¨è³‡æ–™é©…å‹•é¸æ“‡çš„ 10 å€‹å•é¡Œ
    gaps_by_block["faq"],
    all_previous_content
)
print(f"âœ… FAQ å®Œæˆ ({len(faq_data.get('faqs', []))} å€‹å•é¡Œ)")

# ================================================
# è¨ˆç®—å­—æ•¸çµ±è¨ˆ
# ================================================

total_words = sum([
    len(generated_blocks["quick_summary"]),
    len(generated_blocks["definition"]),
    len(generated_blocks["uses"]),
    len(generated_blocks["buying_guide"]),
    len(generated_blocks["maintenance"])
])

faq_words = sum(len(faq.get("answer", "")) for faq in faq_data.get("faqs", []))
total_words += faq_words

print(f"\nğŸ“Š å­—æ•¸çµ±è¨ˆï¼š")
print(f"   Quick Summary: {len(generated_blocks['quick_summary'])} å­—")
print(f"   Definition: {len(generated_blocks['definition'])} å­—")
print(f"   Uses: {len(generated_blocks['uses'])} å­—")
print(f"   Buying Guide: {len(generated_blocks['buying_guide'])} å­—")
print(f"   Maintenance: {len(generated_blocks['maintenance'])} å­—")
print(f"   FAQ: {faq_words} å­— ({len(faq_data.get('faqs', []))} å€‹å•é¡Œ)")
print(f"   ç¸½è¨ˆ: {total_words} å­—")

# ================================================
# ç”Ÿæˆ Markdown æ–‡ç« 
# ================================================

print("\nğŸ“ ç”Ÿæˆ Markdown æ–‡ç« ...")

markdown_content = f"""# {config.topic}

> æœ¬æ–‡æ•´åˆ {len(outlines_data['outlines'])} å€‹æŸ¥è©¢çš„ DSPy åˆ†æçµæœï¼Œæä¾›å®Œæ•´çš„ {config.topic} æŒ‡å—ã€‚

---

## Quick Summary

{generated_blocks["quick_summary"]}

---

## Definition

{generated_blocks["definition"]}

---

## Uses

{generated_blocks["uses"]}

---

## Buying Guide

{generated_blocks["buying_guide"]}

---

## Maintenance

{generated_blocks["maintenance"]}

---

## FAQ - å¸¸è¦‹å•é¡Œ

"""

# æ·»åŠ  FAQ
for i, faq in enumerate(faq_data.get("faqs", []), 1):
    markdown_content += f"### {i}. {faq['question']}\n\n"
    markdown_content += f"{faq['answer']}\n\n"

# æ·»åŠ  SEO å…ƒæ•¸æ“š
markdown_content += f"""---

## SEO å…ƒæ•¸æ“š

**æ–‡ç« çµ±è¨ˆï¼š**
- ç¸½å­—æ•¸ï¼š{total_words}
- ç›®æ¨™ç¯„åœï¼š{config.total_word_count_min}-{config.total_word_count_max}
- H2 æ¨™é¡Œæ•¸ï¼š6 å€‹ï¼ˆ6-block çµæ§‹ï¼‰
- FAQ å•é¡Œæ•¸ï¼š{len(faq_data.get('faqs', []))}

**é—œéµå­—è¦†è“‹ï¼š**
- ä¸»é¡Œï¼š{config.topic} / {config.topic_en}
- åˆ†ææŸ¥è©¢æ•¸ï¼š{len(outlines_data['outlines'])}
- FAQ å•é¡Œé¸æ“‡ï¼šè³‡æ–™é©…å‹•ï¼ˆPAA + è¦å‰‡å¼æå–ï¼‰

**åƒè€ƒä¾†æºï¼š**
"""

for i, ref in enumerate(reference_summaries[:10], 1):
    markdown_content += f"{i}. [{ref['title']}]({ref['url']}) (quality: {ref['quality_score']:.2f}, {ref['lang']})\n"

markdown_content += f"""

---

*ğŸ“… ç”Ÿæˆæ™‚é–“ï¼šè‡ªå‹•ç”Ÿæˆ*
*ğŸ¤– ç”Ÿæˆæ–¹å¼ï¼šåŸºæ–¼ DSPy åˆ†æ + GPT-4o + è³‡æ–™é©…å‹• FAQ é¸æ“‡*
*ğŸ“Š è³‡æ–™ä¾†æºï¼š{len(all_pages)} å€‹é é¢ï¼ˆ{sum(1 for p in all_pages if p.get('lang')=='zh-TW')} ä¸­æ–‡ + {sum(1 for p in all_pages if p.get('lang')=='en')} è‹±æ–‡ï¼‰ + {len(serp_data['serp_results'])} å€‹ SERP åˆ†æ*
"""

# ================================================
# å„²å­˜çµæœ
# ================================================

print("\nğŸ’¾ å„²å­˜æ–‡ç« ...")

# å„²å­˜ Markdown
md_path = config.data_dir / "final_article.md"
with open(md_path, "w", encoding="utf-8") as f:
    f.write(markdown_content)

print(f"âœ… Markdown å·²å„²å­˜ï¼š{md_path}")

# å„²å­˜å…ƒæ•¸æ“š
metadata = {
    "topic_zh": config.topic,
    "topic_en": config.topic_en,
    "structure": "6-block",
    "word_count": {
        "quick_summary": len(generated_blocks["quick_summary"]),
        "definition": len(generated_blocks["definition"]),
        "uses": len(generated_blocks["uses"]),
        "buying_guide": len(generated_blocks["buying_guide"]),
        "maintenance": len(generated_blocks["maintenance"]),
        "faq": faq_words,
        "total": total_words
    },
    "blocks": {
        "h2_count": 6,
        "faq_questions": len(faq_data.get("faqs", []))
    },
    "sources": {
        "queries_analyzed": len(outlines_data['outlines']),
        "pages_extracted": len(all_pages),
        "pages_zh": sum(1 for p in all_pages if p.get('lang') == 'zh-TW'),
        "pages_en": sum(1 for p in all_pages if p.get('lang') == 'en'),
        "serp_results": len(serp_data["serp_results"]),
        "faq_selection": {
            "paa_candidates": len(paa_candidates),
            "extracted_candidates": len(extracted_candidates),
            "total_candidates": len(paa_candidates) + len(extracted_candidates),
            "selected_questions": len(selected_faq_questions)
        },
        "content_gaps": sum(len(gaps) for gaps in gaps_by_block.values())
    },
    "references": [
        {
            "title": ref["title"],
            "url": ref["url"],
            "query": ref["query"]
        }
        for ref in reference_summaries[:10]
    ],
    "seo_requirements": {
        "word_count_range": f"{config.total_word_count_min}-{config.total_word_count_max}",
        "word_count_achieved": total_words,
        "within_range": config.total_word_count_min <= total_words <= config.total_word_count_max
    }
}

metadata_path = config.data_dir / "final_article_metadata.json"
with open(metadata_path, "w", encoding="utf-8") as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print(f"âœ… å…ƒæ•¸æ“šå·²å„²å­˜ï¼š{metadata_path}")

# ================================================
# æœ€çµ‚å ±å‘Š
# ================================================

print("\n" + "=" * 60)
print("âœ… 6-Block æ–‡ç« ç”Ÿæˆå®Œæˆï¼")
print("=" * 60)
print(f"ğŸ“ æ–‡ç« æª”æ¡ˆï¼š{md_path}")
print(f"ğŸ“ å…ƒæ•¸æ“šæª”æ¡ˆï¼š{metadata_path}")
print(f"\nğŸ“Š æ–‡ç« çµ±è¨ˆï¼š")
print(f"   ç¸½å­—æ•¸ï¼š{total_words} å­—")
print(f"   ç›®æ¨™ç¯„åœï¼š{config.total_word_count_min}-{config.total_word_count_max} å­—")
print(f"   é”æˆç‡ï¼š{'âœ… ç¬¦åˆ' if config.total_word_count_min <= total_words <= config.total_word_count_max else 'âš ï¸ è¶…å‡ºç¯„åœ'}")
print(f"\nğŸ“ˆ 6-Block çµæ§‹ï¼š")
print(f"   1. Quick Summaryï¼š{len(generated_blocks['quick_summary'])} å­—")
print(f"   2. Definitionï¼š{len(generated_blocks['definition'])} å­—")
print(f"   3. Usesï¼š{len(generated_blocks['uses'])} å­—")
print(f"   4. Buying Guideï¼š{len(generated_blocks['buying_guide'])} å­—")
print(f"   5. Maintenanceï¼š{len(generated_blocks['maintenance'])} å­—")
print(f"   6. FAQï¼š{faq_words} å­—ï¼ˆ{len(faq_data.get('faqs', []))} å€‹å•é¡Œï¼‰")
print(f"\nğŸ¯ è³‡æ–™ä¾†æºï¼š")
print(f"   åˆ†ææŸ¥è©¢ï¼š{len(outlines_data['outlines'])} å€‹")
print(f"   åƒè€ƒé é¢ï¼š{len(all_pages)} å€‹ï¼ˆä¸­æ–‡ {sum(1 for p in all_pages if p.get('lang')=='zh-TW')} + è‹±æ–‡ {sum(1 for p in all_pages if p.get('lang')=='en')}ï¼‰")
print(f"   Content Gapsï¼š{sum(len(gaps) for gaps in gaps_by_block.values())} å€‹")
print(f"\nğŸ“Š è³‡æ–™é©…å‹•çš„ FAQ é¸æ“‡ï¼š")
print(f"   PAA å€™é¸å•é¡Œï¼š{len(paa_candidates)} å€‹")
print(f"   è¦å‰‡å¼æå–å•é¡Œï¼š{len(extracted_candidates)} å€‹")
print(f"   ç¸½å€™é¸æ± ï¼š{len(paa_candidates) + len(extracted_candidates)} å€‹")
print(f"   æœ€çµ‚é¸æ“‡ï¼š{len(selected_faq_questions)} å€‹ï¼ˆç¶“ Embedding å»é‡ + ä¸»é¡Œéæ¿¾ï¼‰")
print("\n" + "=" * 60)
