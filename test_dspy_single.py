# -*- coding: utf-8 -*-
"""
Test DSPy Analysis with a single query
"""

import os, json, sys, io
from pathlib import Path

# Windows UTF-8 support
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

ROOT_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config
from analyze.dspy_modules import (
    ContentSummarizer,
    GapAnalyzer,
    OutlineGenerator,
    init_dspy
)

print("\nğŸ§ª DSPy Single Query Test")
print("=" * 60)

# åˆå§‹åŒ– DSPy
openai_key = config.get_openai_key()
init_dspy(config.dspy_model_main, openai_key)
print(f"âœ… DSPy åˆå§‹åŒ–å®Œæˆ (model: {config.dspy_model_main})")

# åˆå§‹åŒ–æ¨¡çµ„
summarizer = ContentSummarizer()
gap_analyzer = GapAnalyzer()
outline_generator = OutlineGenerator(block_config=config.article_blocks)
print("âœ… ä¸‰å€‹æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")

# è¼‰å…¥ SERP æ•¸æ“šï¼ˆåªå–ç¬¬ä¸€å€‹æŸ¥è©¢ï¼‰
serp_path = config.data_dir / config.output_files["serp_analysis"]
with open(serp_path, "r", encoding="utf-8") as f:
    serp_data = json.load(f)

# åªè™•ç†ç¬¬ä¸€å€‹æŸ¥è©¢
test_item = serp_data["serp_results"][0]
query = test_item["query"]
serp = test_item["serp_data"]
analysis = test_item["analysis"]

print(f"\nğŸ“Œ æ¸¬è©¦æŸ¥è©¢ï¼š{query}")
print(f"   AISEO è§¸ç™¼ï¼š{analysis['aiseo_triggered']}")
print(f"   PAA æ•¸é‡ï¼š{analysis['paa_count']}")
print(f"   æœ‰æ©Ÿçµæœæ•¸ï¼š{len(serp['organic_results'])}")

# ================================================
# Step 1: ContentSummarizer
# ================================================
print("\n" + "=" * 60)
print("ğŸ“ Step 1: ContentSummarizer")
print("=" * 60)

organic_results = serp.get("organic_results", [])
print(f"è™•ç† {len(organic_results)} å€‹ç«¶çˆ­è€…çµæœ...")

try:
    summaries = summarizer.forward(query, organic_results)
    print(f"âœ… ç¸½çµå®Œæˆï¼š{len(summaries)} å€‹ç«¶çˆ­è€…")

    # é¡¯ç¤ºå‰ 2 å€‹ç¸½çµ
    for i, summary in enumerate(summaries[:2], 1):
        print(f"\nç«¶çˆ­è€… {i}:")
        print(f"  åŸŸåï¼š{summary.domain}")
        print(f"  æ·±åº¦ï¼š{summary.content_depth}")
        print(f"  é—œéµé»ï¼š")
        for point in summary.key_points:
            print(f"    - {point}")

except Exception as e:
    print(f"âŒ å¤±æ•—ï¼š{e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ================================================
# Step 2: GapAnalyzer
# ================================================
print("\n" + "=" * 60)
print("ğŸ” Step 2: GapAnalyzer")
print("=" * 60)

paa_questions = serp.get("people_also_ask", [])
aiseo_triggered = analysis.get("aiseo_triggered", False)

print(f"åˆ†æ {len(summaries)} å€‹ç«¶çˆ­è€… + {len(paa_questions)} å€‹ PAA...")

try:
    gaps = gap_analyzer.forward(
        query=query,
        competitor_summaries=summaries,
        paa_questions=paa_questions,
        aiseo_triggered=aiseo_triggered
    )
    print(f"âœ… æ‰¾åˆ° {len(gaps)} å€‹å…§å®¹ç¼ºå£")

    # é¡¯ç¤ºæ‰€æœ‰ç¼ºå£
    for i, gap in enumerate(gaps, 1):
        print(f"\nç¼ºå£ {i}: [{gap.gap_type}] (åˆ†æ•¸: {gap.opportunity_score:.2f})")
        print(f"  {gap.description}")
        print(f"  å»ºè­°ï¼š{gap.recommended_action}")

except Exception as e:
    print(f"âŒ å¤±æ•—ï¼š{e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ================================================
# Step 3: OutlineGenerator
# ================================================
print("\n" + "=" * 60)
print("ğŸ“‹ Step 3: OutlineGenerator")
print("=" * 60)

print(f"ç”Ÿæˆæ–‡ç« å¤§ç¶±ï¼ˆ4-block çµæ§‹ï¼‰...")

try:
    outline = outline_generator.forward(
        query=query,
        content_gaps=gaps,
        paa_questions=paa_questions,
        aiseo_triggered=aiseo_triggered
    )
    print(f"âœ… å¤§ç¶±ç”Ÿæˆå®Œæˆ")

    # é¡¯ç¤ºå¤§ç¶±çµæ§‹
    if "blocks" in outline:
        print(f"\næ–‡ç« çµæ§‹ï¼š{len(outline['blocks'])} å€‹å€å¡Š")
        for block in outline["blocks"]:
            print(f"\n[{block.get('block_name', 'N/A')}]")
            print(f"  æ¨™é¡Œï¼š{block.get('block_title', 'N/A')}")
            print(f"  å­—æ•¸ï¼š{block.get('word_count_target', 'N/A')}")

            subsections = block.get('subsections', [])
            if subsections:
                print(f"  å­ç« ç¯€æ•¸ï¼š{len(subsections)}")
                for sub in subsections[:2]:  # åªé¡¯ç¤ºå‰ 2 å€‹
                    print(f"    - {sub.get('title', 'N/A')}")

    # å„²å­˜è¼¸å‡º
    output_path = config.data_dir / "test_single_outline.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "query": query,
            "summaries": [s.model_dump() for s in summaries],
            "gaps": [g.model_dump() for g in gaps],
            "outline": outline
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ è¼¸å‡ºå·²å„²å­˜ï¼š{output_path}")

except Exception as e:
    print(f"âŒ å¤±æ•—ï¼š{e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "=" * 60)
print("ğŸ‰ æ¸¬è©¦å®Œæˆï¼æ‰€æœ‰æ¨¡çµ„é‹ä½œæ­£å¸¸")
print("=" * 60)
