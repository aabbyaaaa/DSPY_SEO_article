# -*- coding: utf-8 -*-
"""
ðŸ“Š PAA å•é¡Œåˆ†ä½ˆåˆ†æžè…³æœ¬
åˆ†æž 127 å€‹ PAA å•é¡Œçš„é »çŽ‡ã€èªžè¨€ã€é¡žåž‹ã€èˆ‡å‰ 5 å€‹å€å¡Šçš„ç›¸ä¼¼åº¦
"""

import os
import sys
import io
import json
import re
from pathlib import Path
from typing import List, Dict
from openai import OpenAI

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from config.config_loader import config, load_config

print("\n" + "=" * 60)
print("ðŸ“Š PAA å•é¡Œåˆ†ä½ˆåˆ†æž")
print("=" * 60)

# ================================================
# 1ï¸âƒ£ è¼‰å…¥è³‡æ–™
# ================================================
print("\nðŸ“¦ è¼‰å…¥è³‡æ–™...")

# è¼‰å…¥ article_outlines_bilingual.json
outlines_path = config.data_dir / "article_outlines_bilingual.json"
with open(outlines_path, 'r', encoding='utf-8') as f:
    outlines_data = json.load(f)

# è¼‰å…¥å·²ç”Ÿæˆçš„æ–‡ç« ï¼ˆå‰ 5 å€‹å€å¡Šï¼‰
article_path = config.data_dir / "final_article.md"
if article_path.exists():
    with open(article_path, 'r', encoding='utf-8') as f:
        article_content = f.read()

    # æå–å‰ 5 å€‹å€å¡Šï¼ˆQuick Summary åˆ° Maintenanceï¼‰
    # å‡è¨­ FAQ å€å¡Šåœ¨ "## å¸¸è¦‹å•é¡Œ" ä¹‹å¾Œ
    if "## å¸¸è¦‹å•é¡Œ" in article_content:
        previous_blocks = article_content.split("## å¸¸è¦‹å•é¡Œ")[0]
    else:
        previous_blocks = article_content
else:
    previous_blocks = ""
    print("âš ï¸ æ‰¾ä¸åˆ° final_article.mdï¼Œç„¡æ³•åˆ†æžèˆ‡å‰ 5 å€‹å€å¡Šçš„ç›¸ä¼¼åº¦")

settings = load_config()

print(f"âœ… è¼‰å…¥äº† {len(outlines_data['outlines'])} å€‹æŸ¥è©¢å¤§ç¶±")

# ================================================
# 2ï¸âƒ£ è¨ˆç®— PAA é »çŽ‡
# ================================================
print("\n" + "=" * 60)
print("ðŸ“ˆ PAA é »çŽ‡åˆ†æž")
print("=" * 60)

paa_frequency = {}

for outline_item in outlines_data["outlines"]:
    for paa in outline_item.get("paa_questions", []):
        q_text = paa.get("question", "") if isinstance(paa, dict) else paa

        if q_text:
            if q_text not in paa_frequency:
                # è‡ªå‹•æª¢æ¸¬èªžè¨€
                is_english = bool(re.search(r'[a-zA-Z]{3,}', q_text))

                paa_frequency[q_text] = {
                    "question": q_text,
                    "frequency": 0,
                    "lang": "en" if is_english else "zh-TW"
                }
            paa_frequency[q_text]["frequency"] += 1

paa_candidates = list(paa_frequency.values())

# å‹•æ…‹è¨ˆç®—é–¾å€¼ï¼ˆèˆ‡ stage4 ç›¸åŒé‚è¼¯ï¼‰
freq_distribution = [p["frequency"] for p in paa_candidates]
max_freq = max(freq_distribution)
median_freq = sorted(freq_distribution)[len(freq_distribution) // 2]

high_threshold = max(3, median_freq + 1)
medium_threshold = max(2, median_freq)

# åˆ†å±¤çµ±è¨ˆ
high_freq_questions = []
medium_freq_questions = []
low_freq_questions = []

for paa in paa_candidates:
    freq = paa["frequency"]

    if freq >= high_threshold:
        paa["tier"] = "high"
        paa["base_score"] = 15.0
        high_freq_questions.append(paa)
    elif freq >= medium_threshold:
        paa["tier"] = "medium"
        paa["base_score"] = 12.0
        medium_freq_questions.append(paa)
    else:
        paa["tier"] = "low"
        paa["base_score"] = 8.0
        low_freq_questions.append(paa)

print(f"\nâœ… ç¸½å…± {len(paa_candidates)} å€‹ä¸é‡è¤‡ PAA å•é¡Œ")
print(f"ðŸ“ˆ é »çŽ‡ç¯„åœï¼š{min(freq_distribution)}-{max_freq} æ¬¡")
print(f"ðŸ“Š ä¸­ä½æ•¸ï¼š{median_freq}")
print(f"ðŸŽ¯ å‹•æ…‹é–¾å€¼ï¼šé«˜é » â‰¥{high_threshold}, ä¸­é » â‰¥{medium_threshold}")
print(f"\nðŸ“Š åˆ†å±¤çµ±è¨ˆï¼š")
print(f"   - é«˜é »ï¼ˆbase_score=15ï¼‰ï¼š{len(high_freq_questions)} å€‹")
print(f"   - ä¸­é »ï¼ˆbase_score=12ï¼‰ï¼š{len(medium_freq_questions)} å€‹")
print(f"   - ä½Žé »ï¼ˆbase_score=8ï¼‰ï¼š{len(low_freq_questions)} å€‹")

# é¡¯ç¤ºå‰ 10 é«˜é »å•é¡Œ
paa_candidates.sort(key=lambda x: x["frequency"], reverse=True)
print(f"\nðŸ† å‰ 10 é«˜é » PAA å•é¡Œï¼š")
for i, paa in enumerate(paa_candidates[:10], 1):
    print(f"   {i}. [{paa['lang']}] (é »çŽ‡ {paa['frequency']}, tier: {paa['tier']}) {paa['question'][:70]}...")

# ================================================
# 3ï¸âƒ£ èªžè¨€åˆ†ä½ˆ
# ================================================
print("\n" + "=" * 60)
print("ðŸŒ èªžè¨€åˆ†ä½ˆåˆ†æž")
print("=" * 60)

zh_questions = [p for p in paa_candidates if p['lang'] == 'zh-TW']
en_questions = [p for p in paa_candidates if p['lang'] == 'en']

print(f"\nèªžè¨€åˆ†ä½ˆï¼š")
print(f"   - ä¸­æ–‡ï¼š{len(zh_questions)} å€‹ ({len(zh_questions)/len(paa_candidates)*100:.1f}%)")
print(f"   - è‹±æ–‡ï¼š{len(en_questions)} å€‹ ({len(en_questions)/len(paa_candidates)*100:.1f}%)")

# æŒ‰é »çŽ‡å±¤ç´šåˆ†èªžè¨€
zh_high = [p for p in zh_questions if p['tier'] == 'high']
zh_medium = [p for p in zh_questions if p['tier'] == 'medium']
zh_low = [p for p in zh_questions if p['tier'] == 'low']

en_high = [p for p in en_questions if p['tier'] == 'high']
en_medium = [p for p in en_questions if p['tier'] == 'medium']
en_low = [p for p in en_questions if p['tier'] == 'low']

print(f"\nä¸­æ–‡å•é¡Œåˆ†ä½ˆï¼š")
print(f"   - é«˜é »ï¼š{len(zh_high)} å€‹")
print(f"   - ä¸­é »ï¼š{len(zh_medium)} å€‹")
print(f"   - ä½Žé »ï¼š{len(zh_low)} å€‹")

print(f"\nè‹±æ–‡å•é¡Œåˆ†ä½ˆï¼š")
print(f"   - é«˜é »ï¼š{len(en_high)} å€‹")
print(f"   - ä¸­é »ï¼š{len(en_medium)} å€‹")
print(f"   - ä½Žé »ï¼š{len(en_low)} å€‹")

# ================================================
# 4ï¸âƒ£ å•é¡Œé¡žåž‹åˆ†é¡žï¼ˆç”¨ LLMï¼‰
# ================================================
print("\n" + "=" * 60)
print("ðŸ” å•é¡Œé¡žåž‹åˆ†æžï¼ˆç”¨ LLM è‡ªå‹•åˆ†é¡žï¼‰")
print("=" * 60)

client = OpenAI(api_key=config.get_openai_key())

def classify_question_type(question: str) -> str:
    """ç”¨ GPT-4o-mini åˆ†é¡žå•é¡Œé¡žåž‹"""
    prompt = f"""è«‹å°‡ä»¥ä¸‹å•é¡Œåˆ†é¡žç‚ºä»¥ä¸‹ 5 ç¨®é¡žåž‹ä¹‹ä¸€ï¼š

å•é¡Œï¼š{question}

é¡žåž‹é¸é …ï¼š
1. Howï¼ˆæ“ä½œæ–¹æ³•ï¼‰ï¼šå¦‚ä½•ä½¿ç”¨ã€å¦‚ä½•æ“ä½œã€æ€Žéº¼åš
2. Troubleshootingï¼ˆæ•…éšœæŽ’é™¤ï¼‰ï¼šå•é¡Œè¨ºæ–·ã€éŒ¯èª¤è™•ç†ã€æ•…éšœä¿®å¾©
3. Specificationï¼ˆè¦æ ¼é¸æ“‡ï¼‰ï¼šè¦æ ¼ã€åƒæ•¸ã€æº«åº¦ã€å£“åŠ›ã€å°ºå¯¸
4. Safetyï¼ˆå®‰å…¨æ³¨æ„ï¼‰ï¼šå®‰å…¨äº‹é …ã€æ³¨æ„äº‹é …ã€é¢¨éšªã€é é˜²
5. Comparisonï¼ˆæ¯”è¼ƒå·®ç•°ï¼‰ï¼šæ¯”è¼ƒã€å·®ç•°ã€ä¸åŒã€å€åˆ¥ã€å„ªç¼ºé»ž

åªè¼¸å‡ºé¡žåž‹åç¨±ï¼ˆHow / Troubleshooting / Specification / Safety / Comparisonï¼‰ï¼Œä¸è¦ä»»ä½•èªªæ˜Žã€‚"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=10
        )

        result = response.choices[0].message.content.strip()
        # æ¨™æº–åŒ–è¼¸å‡º
        if "how" in result.lower():
            return "How"
        elif "troubleshoot" in result.lower():
            return "Troubleshooting"
        elif "specification" in result.lower():
            return "Specification"
        elif "safety" in result.lower():
            return "Safety"
        elif "comparison" in result.lower():
            return "Comparison"
        else:
            return "Other"
    except Exception as e:
        print(f"   âš ï¸ åˆ†é¡žå¤±æ•—ï¼š{e}")
        return "Unknown"

print(f"\né–‹å§‹åˆ†é¡ž {len(paa_candidates)} å€‹å•é¡Œ...")
print(f"ï¼ˆé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼Œä½¿ç”¨ gpt-4o-miniï¼‰")

type_distribution = {
    "How": [],
    "Troubleshooting": [],
    "Specification": [],
    "Safety": [],
    "Comparison": [],
    "Other": []
}

for i, paa in enumerate(paa_candidates, 1):
    q_type = classify_question_type(paa['question'])
    paa['type'] = q_type
    type_distribution[q_type].append(paa)

    if i % 20 == 0:
        print(f"   é€²åº¦ï¼š{i}/{len(paa_candidates)}")

print(f"\nâœ… åˆ†é¡žå®Œæˆï¼")
print(f"\nðŸ“Š å•é¡Œé¡žåž‹åˆ†ä½ˆï¼š")
for q_type, questions in type_distribution.items():
    if questions:
        print(f"   - {q_type}ï¼š{len(questions)} å€‹ ({len(questions)/len(paa_candidates)*100:.1f}%)")
        # é¡¯ç¤ºè©²é¡žåž‹çš„å‰ 3 å€‹å•é¡Œ
        print(f"      ç¯„ä¾‹ï¼š")
        for j, q in enumerate(questions[:3], 1):
            print(f"         {j}. [{q['lang']}] (é »çŽ‡ {q['frequency']}) {q['question'][:50]}...")

# ================================================
# 5ï¸âƒ£ èˆ‡å‰ 5 å€‹å€å¡Šçš„ç›¸ä¼¼åº¦åˆ†æž
# ================================================
if previous_blocks:
    print("\n" + "=" * 60)
    print("ðŸš« èˆ‡å‰ 5 å€‹å€å¡Šé‡è¤‡åº¦åˆ†æž")
    print("=" * 60)

    # å°‡å‰ 5 å€‹å€å¡Šæ‹†æˆå¥å­
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', previous_blocks)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

    print(f"\nå‰ 5 å€‹å€å¡Šå¥å­æ•¸ï¼š{len(sentences)} å€‹")

    # è¨ˆç®— Embedding
    questions = [p["question"] for p in paa_candidates]
    all_texts = questions + sentences

    print(f"è¨ˆç®— {len(all_texts)} å€‹æ–‡æœ¬çš„ Embeddings...")

    try:
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=all_texts
        )
        embeddings = [item.embedding for item in response.data]

        question_embeddings = embeddings[:len(questions)]
        sentence_embeddings = embeddings[len(questions):]

        print(f"âœ… Embedding è¨ˆç®—å®Œæˆ")

        # è¨ˆç®—æ¯å€‹å•é¡Œèˆ‡å‰é¢å€å¡Šçš„æœ€å¤§ç›¸ä¼¼åº¦
        import numpy as np

        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

        for i, paa in enumerate(paa_candidates):
            max_similarity = 0

            for sent_emb in sentence_embeddings:
                similarity = cosine_similarity(question_embeddings[i], sent_emb)
                max_similarity = max(max_similarity, similarity)

            paa['max_similarity'] = max_similarity

        # çµ±è¨ˆç›¸ä¼¼åº¦åˆ†ä½ˆï¼ˆthreshold=0.7ï¼‰
        threshold = 0.7
        would_be_filtered = [p for p in paa_candidates if p['max_similarity'] >= threshold]
        would_pass = [p for p in paa_candidates if p['max_similarity'] < threshold]

        print(f"\nðŸ“Š ç›¸ä¼¼åº¦åˆ†ä½ˆï¼ˆthreshold={threshold}ï¼‰ï¼š")
        print(f"   - æœƒè¢«éŽæ¿¾ï¼ˆâ‰¥{threshold}ï¼‰ï¼š{len(would_be_filtered)} å€‹ ({len(would_be_filtered)/len(paa_candidates)*100:.1f}%)")
        print(f"   - æœƒé€šéŽï¼ˆ<{threshold}ï¼‰ï¼š{len(would_pass)} å€‹ ({len(would_pass)/len(paa_candidates)*100:.1f}%)")

        # é¡¯ç¤ºæœƒè¢«éŽæ¿¾çš„é«˜é »å•é¡Œï¼ˆé€™äº›å¾ˆå¯æƒœï¼‰
        high_freq_filtered = [p for p in would_be_filtered if p['tier'] == 'high']
        if high_freq_filtered:
            print(f"\nâš ï¸ æœƒè¢«éŽæ¿¾çš„é«˜é »å•é¡Œï¼ˆ{len(high_freq_filtered)} å€‹ï¼‰ï¼š")
            for i, paa in enumerate(high_freq_filtered[:5], 1):
                print(f"   {i}. (é »çŽ‡ {paa['frequency']}, ç›¸ä¼¼åº¦ {paa['max_similarity']:.2f}) {paa['question'][:60]}...")

        # é¡¯ç¤ºæœƒé€šéŽçš„é«˜é »å•é¡Œï¼ˆé€™äº›æ˜¯å¥½å€™é¸ï¼‰
        high_freq_pass = [p for p in would_pass if p['tier'] == 'high']
        if high_freq_pass:
            print(f"\nâœ… æœƒé€šéŽçš„é«˜é »å•é¡Œï¼ˆ{len(high_freq_pass)} å€‹ï¼‰ï¼š")
            high_freq_pass.sort(key=lambda x: x['frequency'], reverse=True)
            for i, paa in enumerate(high_freq_pass[:10], 1):
                print(f"   {i}. (é »çŽ‡ {paa['frequency']}, ç›¸ä¼¼åº¦ {paa['max_similarity']:.2f}, {paa['type']}) {paa['question'][:60]}...")

    except Exception as e:
        print(f"âŒ Embedding è¨ˆç®—å¤±æ•—ï¼š{e}")

# ================================================
# 6ï¸âƒ£ æœ€çµ‚å€™é¸æ± é æ¸¬
# ================================================
print("\n" + "=" * 60)
print("ðŸŽ¯ æœ€çµ‚å€™é¸æ± é æ¸¬ï¼ˆåŸºæ–¼ä½ çš„ç­–ç•¥ï¼‰")
print("=" * 60)

print(f"\nä½ çš„ç­–ç•¥ï¼š")
print(f"   1ï¸âƒ£ ä¸»é¡Œæ–°ç©Žæ€§ï¼ˆæœ€é‡è¦ï¼‰ï¼šçµ•å°ä¸èƒ½èˆ‡å‰ 5 å€‹å€å¡Šé‡è¤‡ï¼ˆthreshold=0.7ï¼‰")
print(f"   2ï¸âƒ£ é »çŽ‡ï¼ˆGoogle èªç‚ºé‡è¦ï¼‰ï¼šé«˜é » PAA å„ªå…ˆ")
print(f"   3ï¸âƒ£ å¯¦ç”¨æ€§ï¼ˆLLM è¦ºå¾—é‡è¦ï¼‰ï¼šå•é¡Œå®Œæ•´ä¸”ç¬¦åˆç”¨æˆ¶éœ€æ±‚")

if previous_blocks:
    # éŽæ¿¾èˆ‡å‰ 5 å€‹å€å¡Šé‡è¤‡çš„å•é¡Œ
    final_candidates = [p for p in paa_candidates if p.get('max_similarity', 0) < 0.7]

    print(f"\nðŸ“Š å€™é¸æ± ç‹€æ…‹ï¼š")
    print(f"   - åŽŸå§‹ PAA å•é¡Œï¼š{len(paa_candidates)} å€‹")
    print(f"   - éŽæ¿¾é‡è¤‡å¾Œï¼š{len(final_candidates)} å€‹")

    # æŒ‰é »çŽ‡æŽ’åº
    final_candidates.sort(key=lambda x: x['frequency'], reverse=True)

    print(f"\nðŸ† é æ¸¬çš„å‰ 10 å€‹ FAQ å•é¡Œï¼ˆæŒ‰é »çŽ‡æŽ’åºï¼‰ï¼š")
    for i, paa in enumerate(final_candidates[:10], 1):
        print(f"\n   {i}. [{paa['lang']}] (é »çŽ‡ {paa['frequency']}, {paa['type']}, base_score {paa['base_score']:.1f})")
        print(f"      {paa['question']}")
        print(f"      ç›¸ä¼¼åº¦: {paa.get('max_similarity', 0):.2f}")
else:
    print(f"\nâš ï¸ ç„¡æ³•é æ¸¬æœ€çµ‚å€™é¸æ± ï¼ˆç¼ºå°‘å‰ 5 å€‹å€å¡Šè³‡æ–™ï¼‰")

# ================================================
# 7ï¸âƒ£ å„²å­˜åˆ†æžçµæžœ
# ================================================
print("\n" + "=" * 60)
print("ðŸ’¾ å„²å­˜åˆ†æžçµæžœ")
print("=" * 60)

output_path = config.data_dir / "paa_distribution_analysis.json"

analysis_result = {
    "total_paa_questions": len(paa_candidates),
    "frequency_distribution": {
        "max": max_freq,
        "median": median_freq,
        "high_threshold": high_threshold,
        "medium_threshold": medium_threshold,
        "high_freq_count": len(high_freq_questions),
        "medium_freq_count": len(medium_freq_questions),
        "low_freq_count": len(low_freq_questions)
    },
    "language_distribution": {
        "zh_TW": len(zh_questions),
        "en": len(en_questions),
        "zh_TW_percentage": len(zh_questions)/len(paa_candidates)*100,
        "en_percentage": len(en_questions)/len(paa_candidates)*100
    },
    "type_distribution": {
        q_type: len(questions) for q_type, questions in type_distribution.items()
    },
    "similarity_analysis": {
        "threshold": 0.7,
        "would_be_filtered": len(would_be_filtered) if previous_blocks else None,
        "would_pass": len(would_pass) if previous_blocks else None
    } if previous_blocks else None,
    "top_10_predicted": [
        {
            "question": p['question'],
            "frequency": p['frequency'],
            "lang": p['lang'],
            "type": p.get('type', 'Unknown'),
            "tier": p['tier'],
            "base_score": p['base_score'],
            "max_similarity": p.get('max_similarity', 0)
        }
        for p in (final_candidates[:10] if previous_blocks else paa_candidates[:10])
    ],
    "all_paa_questions": [
        {
            "question": p['question'],
            "frequency": p['frequency'],
            "lang": p['lang'],
            "type": p.get('type', 'Unknown'),
            "tier": p['tier'],
            "base_score": p['base_score'],
            "max_similarity": p.get('max_similarity', 0)
        }
        for p in paa_candidates
    ]
}

with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(analysis_result, f, ensure_ascii=False, indent=2)

print(f"\nâœ… åˆ†æžçµæžœå·²å„²å­˜è‡³ï¼š{output_path}")

print("\n" + "=" * 60)
print("ðŸŽ‰ åˆ†æžå®Œæˆï¼")
print("=" * 60)
