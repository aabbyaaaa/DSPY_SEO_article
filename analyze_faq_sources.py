# -*- coding: utf-8 -*-
"""
åˆ†æ FAQ å•é¡Œä¾†æº
æª¢æŸ¥è¦å‰‡å¼æå–èƒ½æå–å¤šå°‘å•é¡Œ
"""

import os
import sys
import io
import json
import re
from pathlib import Path

# Windows UTF-8 æ”¯æ´
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

print("\n" + "=" * 60)
print("ğŸ“Š FAQ å•é¡Œä¾†æºåˆ†æ")
print("=" * 60)

# ================================================
# 1ï¸âƒ£ è¼‰å…¥è³‡æ–™
# ================================================
print("\nğŸ“¦ è¼‰å…¥è³‡æ–™...")

# cache_extracted_content.json
content_path = ROOT_DIR / "data" / "cache_extracted_content.json"
with open(content_path, 'r', encoding='utf-8') as f:
    content_data_raw = json.load(f)

# article_outlines_bilingual.json
outlines_path = ROOT_DIR / "data" / "article_outlines_bilingual.json"
with open(outlines_path, 'r', encoding='utf-8') as f:
    outlines_data = json.load(f)

# å°‡ content_data è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
all_pages = []
for lang_key, pages_list in content_data_raw.items():
    if isinstance(pages_list, list):
        all_pages.extend(pages_list)

print(f"âœ… è¼‰å…¥äº† {len(all_pages)} å€‹é é¢")
print(f"âœ… è¼‰å…¥äº† {len(outlines_data['outlines'])} å€‹æŸ¥è©¢å¤§ç¶±")

# ================================================
# 2ï¸âƒ£ åˆ†æ Extracted Content
# ================================================
print("\n" + "=" * 60)
print("ğŸ“„ Extracted Content åˆ†æ")
print("=" * 60)

# èªè¨€åˆ†ä½ˆ
lang_dist = {}
quality_by_lang = {"zh-TW": [], "en": []}

for page in all_pages:
    lang = page.get('lang', 'unknown')
    quality = page.get('quality_score', 0)

    lang_dist[lang] = lang_dist.get(lang, 0) + 1
    if lang in quality_by_lang:
        quality_by_lang[lang].append(quality)

print(f"\nèªè¨€åˆ†ä½ˆï¼š")
for lang, count in lang_dist.items():
    avg_quality = sum(quality_by_lang.get(lang, [0])) / len(quality_by_lang.get(lang, [1]))
    print(f"  - {lang}: {count} é  (å¹³å‡ quality_score: {avg_quality:.2f})")

# ================================================
# 3ï¸âƒ£ è¦å‰‡å¼æå–æ¸¬è©¦ï¼ˆä¸­æ–‡ï¼‰
# ================================================
print("\n" + "=" * 60)
print("ğŸ” è¦å‰‡å¼æå–æ¸¬è©¦ï¼ˆä¸­æ–‡ï¼‰")
print("=" * 60)

patterns_zh = [
    (r"(ç‚ºä»€éº¼[^ï¼Ÿ\n]{5,50}ï¼Ÿ)", "why"),
    (r"(å¦‚ä½•[^ï¼Ÿ\n]{5,50}ï¼Ÿ)", "how"),
    (r"(ä»€éº¼æ˜¯[^ï¼Ÿ\n]{5,50}ï¼Ÿ)", "what"),
    (r"([^ï¼Ÿ\n]{5,50}å—ï¼Ÿ)", "yes_no"),
    (r"Q[:ï¼š]\s*([^ï¼Ÿ\n]+ï¼Ÿ)", "faq"),
    (r"å•[:ï¼š]\s*([^ï¼Ÿ\n]+ï¼Ÿ)", "faq"),
]

extracted_zh = []

for page in all_pages:
    if page.get('lang') != 'zh-TW':
        continue

    content = page['content']
    quality = page.get('quality_score', 0.5)

    for pattern, q_type in patterns_zh:
        matches = re.findall(pattern, content)
        for match in matches:
            question_text = match.strip()
            if 10 < len(question_text) < 100:
                extracted_zh.append({
                    "question": question_text,
                    "type": q_type,
                    "quality_score": quality,
                    "source_url": page['url'][:60] + "..."
                })

print(f"âœ… æå–åˆ° {len(extracted_zh)} å€‹ä¸­æ–‡å•é¡Œ")

# é¡¯ç¤ºå‰ 10 å€‹ï¼ˆæŒ‰ quality_score æ’åºï¼‰
extracted_zh.sort(key=lambda x: x['quality_score'], reverse=True)
print(f"\nå‰ 10 å€‹é«˜å“è³ªä¸­æ–‡å•é¡Œï¼š")
for i, q in enumerate(extracted_zh[:10], 1):
    print(f"\n  [{i}] ({q['type']}, quality: {q['quality_score']:.2f})")
    print(f"      {q['question']}")
    print(f"      ä¾†æº: {q['source_url']}")

# ================================================
# 4ï¸âƒ£ è¦å‰‡å¼æå–æ¸¬è©¦ï¼ˆè‹±æ–‡ï¼‰
# ================================================
print("\n" + "=" * 60)
print("ğŸ” è¦å‰‡å¼æå–æ¸¬è©¦ï¼ˆè‹±æ–‡ï¼‰")
print("=" * 60)

patterns_en = [
    (r"(Why [^?\n]{10,80}\?)", "why"),
    (r"(How [^?\n]{10,80}\?)", "how"),
    (r"(What is [^?\n]{10,80}\?)", "what"),
    (r"(Can [^?\n]{10,80}\?)", "yes_no"),
    (r"Q[:]\s*([^?\n]+\?)", "faq"),
]

extracted_en = []

for page in all_pages:
    if page.get('lang') != 'en':
        continue

    content = page['content']
    quality = page.get('quality_score', 0.5)

    for pattern, q_type in patterns_en:
        matches = re.findall(pattern, content, re.IGNORECASE)
        for match in matches:
            question_text = match.strip()
            if 10 < len(question_text) < 100:
                extracted_en.append({
                    "question": question_text,
                    "type": q_type,
                    "quality_score": quality,
                    "source_url": page['url'][:60] + "..."
                })

print(f"âœ… æå–åˆ° {len(extracted_en)} å€‹è‹±æ–‡å•é¡Œ")

# é¡¯ç¤ºå‰ 10 å€‹ï¼ˆæŒ‰ quality_score æ’åºï¼‰
extracted_en.sort(key=lambda x: x['quality_score'], reverse=True)
print(f"\nå‰ 10 å€‹é«˜å“è³ªè‹±æ–‡å•é¡Œï¼š")
for i, q in enumerate(extracted_en[:10], 1):
    print(f"\n  [{i}] ({q['type']}, quality: {q['quality_score']:.2f})")
    print(f"      {q['question']}")
    print(f"      ä¾†æº: {q['source_url']}")

# ================================================
# 5ï¸âƒ£ PAA å•é¡Œçµ±è¨ˆ
# ================================================
print("\n" + "=" * 60)
print("ğŸ“ PAA å•é¡Œçµ±è¨ˆ")
print("=" * 60)

paa_frequency = {}

for outline_item in outlines_data["outlines"]:
    for paa in outline_item.get("paa_questions", []):
        q_text = paa.get("question", "") if isinstance(paa, dict) else paa

        if q_text:
            if q_text not in paa_frequency:
                paa_frequency[q_text] = {
                    "question": q_text,
                    "frequency": 0
                }
            paa_frequency[q_text]["frequency"] += 1

paa_candidates = list(paa_frequency.values())
paa_candidates.sort(key=lambda x: x["frequency"], reverse=True)

print(f"âœ… æ”¶é›†åˆ° {len(paa_candidates)} å€‹ä¸é‡è¤‡çš„ PAA å•é¡Œ")
print(f"\nå‰ 15 å€‹é«˜é » PAA å•é¡Œï¼š")
for i, paa in enumerate(paa_candidates[:15], 1):
    print(f"  [{i}] (é »ç‡: {paa['frequency']}) {paa['question']}")

# ================================================
# 6ï¸âƒ£ ç¸½çµ
# ================================================
print("\n" + "=" * 60)
print("ğŸ“Š ç¸½çµ")
print("=" * 60)

print(f"\nå€™é¸å•é¡Œæ± çµ„æˆï¼š")
print(f"  - PAA å•é¡Œï¼š{len(paa_candidates)} å€‹")
print(f"  - è¦å‰‡å¼æå–ï¼ˆä¸­æ–‡ï¼‰ï¼š{len(extracted_zh)} å€‹")
print(f"  - è¦å‰‡å¼æå–ï¼ˆè‹±æ–‡ï¼‰ï¼š{len(extracted_en)} å€‹")
print(f"  - ç¸½å€™é¸æ± ï¼š{len(paa_candidates) + len(extracted_zh) + len(extracted_en)} å€‹")

print(f"\nå“è³ªåˆ†ä½ˆï¼š")
print(f"  - ä¸­æ–‡å¹³å‡ quality_score: {sum([q['quality_score'] for q in extracted_zh]) / len(extracted_zh) if extracted_zh else 0:.2f}")
print(f"  - è‹±æ–‡å¹³å‡ quality_score: {sum([q['quality_score'] for q in extracted_en]) / len(extracted_en) if extracted_en else 0:.2f}")

print("\n" + "=" * 60)
print("ğŸ‰ åˆ†æå®Œæˆï¼")
print("=" * 60)
